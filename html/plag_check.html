<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.9.1" />
<title>plag_check API documentation</title>
<meta name="description" content="" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>plag_check</code></h1>
</header>
<section id="section-intro">
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.stem import PorterStemmer
from nltk.stem import WordNetLemmatizer

from os.path import isfile
from os.path import join

import os
from num2words import num2words
import numpy as np
import string
import pandas as pd
import math
import time

def lowercase(data):
    &#34;&#34;&#34;changes the case of all characters in the document to lowercase&#34;&#34;&#34;
    return np.char.lower(data)


def remove_stopwords(data):
    &#34;&#34;&#34;removes stopwords from the document&#34;&#34;&#34;
    stop_words = stopwords.words(&#39;english&#39;)
    words = word_tokenize(str(data))
    new = &#34;&#34;
    for word in words:
        if word not in stop_words and len(word) &gt; 1:
            new = new + &#34; &#34; + word
    return new


def remove_punct(data):
    &#34;&#34;&#34;removes all punctuation from the document&#34;&#34;&#34;
    punct = &#34;!\&#34;#$%&amp;()*+-./:;&lt;=&gt;?@[\]^_`{|}~\n&#34;
    for i in range(len(punct)):
        data = np.char.replace(data, punct[i], &#39; &#39;)
        data = np.char.replace(data, &#34; &#34;, &#34; &#34;)
    data = np.char.replace(data, &#39;,&#39;, &#39;&#39;)
    return data


def remove_apostrophes(data):
    &#34;&#34;&#34;removing apostrophes separately&#34;&#34;&#34;
    data = np.char.replace(data, &#34;&#39;&#34;, &#34;&#34;)
    data = np.char.replace(data, &#34;창\x80\x98&#34;, &#34;&#34;) #removing unicode apostrophes
    data = np.char.replace(data, &#34;창\x80\x99&#34;, &#34;&#34;)
    return data


def stemming(data):
    &#34;&#34;&#34;performing stemming on the tokens in the document&#34;&#34;&#34;
    stemmer = PorterStemmer()
    tokens = word_tokenize(str(data))
    new = &#34;&#34;
    for word in tokens:
        new = new + &#34; &#34; + stemmer.stem(word)
    return new


def lemmatize(data):
    &#34;&#34;&#34;lemmatizing the document&#34;&#34;&#34;
    lemmatizer = WordNetLemmatizer()
    tokens = word_tokenize(str(data))
    new = &#34;&#34;
    for word in tokens:
        new = new + &#34; &#34; + lemmatizer.lemmatize(word)
    return new


def num_to_words(data):
    &#34;&#34;&#34;converting nunmbers to words in the document&#34;&#34;&#34;
    tokens = word_tokenize(str(data))
    new = &#34;&#34;
    for word in tokens:
        try:
            word = num2words(int(w))
        except:
            a = 0
        new = new + &#34; &#34; + word
    new = np.char.replace(new, &#34;-&#34;, &#34; &#34;)
    return new


def normalize(data):
    &#34;&#34;&#34;combining all the preprocessing functions in a suitable order&#34;&#34;&#34;
    data = lowercase(data)
    data = remove_punct(data)
    data = remove_apostrophes(data)
    data = remove_stopwords(data)
    data = num_to_words(data)
    data = lemmatize(data)
    data = stemming(data)
    data = remove_punct(data)
    data = num_to_words(data)
    data = lemmatize(data)
    data = stemming(data)
    data = remove_punct(data) #done again to remove hyphens produced by num2words
    data = remove_stopwords(data) #done agan to remove stopwords produced by num2words
    return data

# END OF NORMALIZATION RELATED FUNCTIONS

#computing tf dictionary

def calcTFdict(doc):
    &#34;&#34;&#34;Returns a term frequency dictionary for each document,
    with keys that are unique tokens in the document and values are the corresponding term frequencies&#34;&#34;&#34;

    TFDict = {}

    #counts number of appearances of term in document
    for term in doc:
        if term in TFDict.keys():
            TFDict[term] +=1
        else:
            TFDict[term] = 1

    #Computing tf for each term
    for key in TFDict:
        TFDict[key] = TFDict[key]/len(doc)

    return TFDict


def calcCountDict(TFdict):
    &#34;&#34;&#34;Returns dictionary with keys as all the unique terms in corpus and values
    is the number of documents in which each term appears&#34;&#34;&#34;

    countDict = {}

    for doc in TFdict:
        for term in doc:
            if term in countDict:
                countDict[term] +=1
            else:
                countDict[term] = 1

    return countDict


#computing idf dictionary

def calcIDFDict(countDict, numfiles):
    &#34;&#34;&#34;Returns dictionary whose keys are all unique words in dataset and
    values are corresponding Inverse Document Frequencies&#34;&#34;&#34;

    IDFDict = {}
    for term in countDict:
        IDFDict[term] = math.log(numfiles / countDict[term])

    return IDFDict


#calculating TF-IDF dictionary
def calcTFIDFDict(TFDict, IDFDict):
    &#34;&#34;&#34;Returns dictionary whose keys are all unique terms in the document
    and values are corresponding TF-IDF value&#34;&#34;&#34;

    TFIDFDict = {}

    #for each term in the document, multiply the tf and idf values

    for term in TFDict:
        TFIDFDict[term] = TFDict[term] * IDFDict[term]

    return TFIDFDict


def calc_TF_IDF_Vector(doc, termDict):
    &#34;&#34;&#34;Creating TF-IDF vector (for calculating cosine similarity)&#34;&#34;&#34;
    TFIDFVec = [0.0] * len(termDict)

    #for each unique term, if it is in the document, store the TF-IDF value
    for i, term in enumerate(termDict):
        if term in doc:
            TFIDFVec[i] = doc[term]

    return TFIDFVec


def dot_product(a, b):
    &#34;&#34;&#34;returns dot product of two vectors&#34;&#34;&#34;
    dp = 0.0
    for i, j in zip(a, b):
        dp += i * j
    return dp


def norm(vec):
    &#34;&#34;&#34;returns the norm or magnitude of a vector&#34;&#34;&#34;
    n = 0.0
    for i in vec:
        n += math.pow(i, 2)
    return math.sqrt(n)


def cosine_similarity(a, b):
    &#34;&#34;&#34;returns cosine similarity score of two vectors&#34;&#34;&#34;
    cs = dot_product(a, b)/(norm(a) * norm(b))
    return cs


start_time = time.time()
normalized_trg = []
normalized_test = []
path_trg = &#34;./texts/&#34; #directory in which training set is located
path_test = &#34;./test/&#34;
#test_file = input(&#34;Enter file name: &#34;) #g4pC_taska.txt
trg_files = [document for document in os.listdir(path_trg) if document.endswith(&#39;.txt&#39;)]
test_files = [document for document in os.listdir(path_test) if document.endswith(&#39;.txt&#39;)]


numfiles_trg = 0 #number of files in the training directory
for file in trg_files:
    file.encode(&#39;utf8&#39;).strip() #encodes each of the files into utf-8
    fh = open(os.path.join(path_trg, file), &#39;r&#39;, encoding = &#34;utf-8&#34;)
    file_content = fh.read()
    numfiles_trg = numfiles_trg + 1

    normalized_trg.append(word_tokenize(str(normalize(file_content)))) #performing normalization on the training files


numfiles_test = 0 #number of files in the testing directory
for file in test_files:
    file.encode(&#39;utf8&#39;) #encodes each of the files into utf-8
    fh = open(os.path.join(path_test, file), &#39;r&#39;, encoding = &#34;utf-8&#34;)
    file_content = fh.read()
    numfiles_test = numfiles_test + 1

    normalized_test.append(word_tokenize(str(normalize(file_content)))) #performing normalization on the test files


normalize_time = time.time() - start_time
&#34;&#34;&#34;storing the time taken for normalizaton&#34;&#34;&#34;


print(&#34;Normalizing time:&#34;, normalize_time, &#34;seconds&#34;)


exec_start_time = time.time()
&#34;&#34;&#34;storing the time at which execution starts&#34;&#34;&#34;

#adding test file to the total corpus so that we can perform TF-IDF vectorization
normalized_corpus = normalized_trg + normalized_test
test_doc_index_start = len(normalized_corpus) - numfiles_test
numfiles = numfiles_trg + numfiles_test


TFdict = [] #term frequency dictionary of the corpus
for i in range(len(normalized_corpus)):
    d = calcTFdict(normalized_corpus[i])
    TFdict.append(d)


countDict = calcCountDict(TFdict)
#calculating the number of documents in which each term appears


IDFDict = calcIDFDict(countDict, numfiles)
#calculating the IDF dictionary of the corpus


TFIDFDict = [calcTFIDFDict(doc, IDFDict) for doc in TFdict]
#calculating the TF-IDF dictionary


termDict = sorted(countDict.keys())


tf_idf_vector = [calc_TF_IDF_Vector(doc, termDict) for doc in TFIDFDict]
#vectorizing the TF-IDF dictionary for the corpus


similarity_scores = {new_list: [] for new_list in range(numfiles_test)}
&#34;&#34;&#34;calculating the cosine similarity of each of the documents in the training set with respect to the test document&#34;&#34;&#34;
for i in range(len(tf_idf_vector) - numfiles_test):
    for j in range(numfiles_test):
        cs = cosine_similarity(tf_idf_vector[(test_doc_index_start + j)], tf_idf_vector[i])
        similarity_scores[j].append(cs)


# THE NEXT FEW CELLS ARE FOR PRINTING THE RANKED LIST FOR EACH OF THE TEST FILES

ranked_dict_keys = trg_files
ranked_dict = {new_list: [] for new_list in range(len(ranked_dict_keys))}


for i in range(len(similarity_scores)):
    ranked_dict_values = similarity_scores[i]
    ranked_dict[i] = {ranked_dict_keys[i]: ranked_dict_values[i] for i in range(len(ranked_dict_keys))}


ranked_dict_view = {new_list: [] for new_list in range(len(similarity_scores))}
for i in range(len(ranked_dict)):
    if(ranked_dict[i]):
        ranked_dict_view[i] = [ (v,k) for k,v in ranked_dict[i].items() ]
        ranked_dict_view[i].sort(reverse = True) # natively sort tuples by first element

        print(&#34;\n&#34;, test_files[i], &#34;similarity ranking:\n&#34;)
        for v,k in ranked_dict_view[i]:
            print(k, &#34;:&#34;, str((v * 100)) + &#34;%&#34;, &#34;similarity&#34;)
    else:
        break

exec_time = time.time() - exec_start_time
&#34;&#34;&#34;storing the executon time&#34;&#34;&#34;
print(&#34;Execution time: &#34;, exec_time, &#34;seconds&#34;)</code></pre>
</details>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-variables">Global variables</h2>
<dl>
<dt id="plag_check.exec_start_time"><code class="name">var <span class="ident">exec_start_time</span></code></dt>
<dd>
<div class="desc"><p>storing the time at which execution starts</p></div>
</dd>
<dt id="plag_check.exec_time"><code class="name">var <span class="ident">exec_time</span></code></dt>
<dd>
<div class="desc"><p>storing the executon time</p></div>
</dd>
<dt id="plag_check.normalize_time"><code class="name">var <span class="ident">normalize_time</span></code></dt>
<dd>
<div class="desc"><p>storing the time taken for normalizaton</p></div>
</dd>
<dt id="plag_check.similarity_scores"><code class="name">var <span class="ident">similarity_scores</span></code></dt>
<dd>
<div class="desc"><p>calculating the cosine similarity of each of the documents in the training set with respect to the test document</p></div>
</dd>
</dl>
</section>
<section>
<h2 class="section-title" id="header-functions">Functions</h2>
<dl>
<dt id="plag_check.calcCountDict"><code class="name flex">
<span>def <span class="ident">calcCountDict</span></span>(<span>TFdict)</span>
</code></dt>
<dd>
<div class="desc"><p>Returns dictionary with keys as all the unique terms in corpus and values
is the number of documents in which each term appears</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def calcCountDict(TFdict):
    &#34;&#34;&#34;Returns dictionary with keys as all the unique terms in corpus and values
    is the number of documents in which each term appears&#34;&#34;&#34;

    countDict = {}

    for doc in TFdict:
        for term in doc:
            if term in countDict:
                countDict[term] +=1
            else:
                countDict[term] = 1

    return countDict</code></pre>
</details>
</dd>
<dt id="plag_check.calcIDFDict"><code class="name flex">
<span>def <span class="ident">calcIDFDict</span></span>(<span>countDict, numfiles)</span>
</code></dt>
<dd>
<div class="desc"><p>Returns dictionary whose keys are all unique words in dataset and
values are corresponding Inverse Document Frequencies</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def calcIDFDict(countDict, numfiles):
    &#34;&#34;&#34;Returns dictionary whose keys are all unique words in dataset and
    values are corresponding Inverse Document Frequencies&#34;&#34;&#34;

    IDFDict = {}
    for term in countDict:
        IDFDict[term] = math.log(numfiles / countDict[term])

    return IDFDict</code></pre>
</details>
</dd>
<dt id="plag_check.calcTFIDFDict"><code class="name flex">
<span>def <span class="ident">calcTFIDFDict</span></span>(<span>TFDict, IDFDict)</span>
</code></dt>
<dd>
<div class="desc"><p>Returns dictionary whose keys are all unique terms in the document
and values are corresponding TF-IDF value</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def calcTFIDFDict(TFDict, IDFDict):
    &#34;&#34;&#34;Returns dictionary whose keys are all unique terms in the document
    and values are corresponding TF-IDF value&#34;&#34;&#34;

    TFIDFDict = {}

    #for each term in the document, multiply the tf and idf values

    for term in TFDict:
        TFIDFDict[term] = TFDict[term] * IDFDict[term]

    return TFIDFDict</code></pre>
</details>
</dd>
<dt id="plag_check.calcTFdict"><code class="name flex">
<span>def <span class="ident">calcTFdict</span></span>(<span>doc)</span>
</code></dt>
<dd>
<div class="desc"><p>Returns a term frequency dictionary for each document,
with keys that are unique tokens in the document and values are the corresponding term frequencies</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def calcTFdict(doc):
    &#34;&#34;&#34;Returns a term frequency dictionary for each document,
    with keys that are unique tokens in the document and values are the corresponding term frequencies&#34;&#34;&#34;

    TFDict = {}

    #counts number of appearances of term in document
    for term in doc:
        if term in TFDict.keys():
            TFDict[term] +=1
        else:
            TFDict[term] = 1

    #Computing tf for each term
    for key in TFDict:
        TFDict[key] = TFDict[key]/len(doc)

    return TFDict</code></pre>
</details>
</dd>
<dt id="plag_check.calc_TF_IDF_Vector"><code class="name flex">
<span>def <span class="ident">calc_TF_IDF_Vector</span></span>(<span>doc, termDict)</span>
</code></dt>
<dd>
<div class="desc"><p>Creating TF-IDF vector (for calculating cosine similarity)</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def calc_TF_IDF_Vector(doc, termDict):
    &#34;&#34;&#34;Creating TF-IDF vector (for calculating cosine similarity)&#34;&#34;&#34;
    TFIDFVec = [0.0] * len(termDict)

    #for each unique term, if it is in the document, store the TF-IDF value
    for i, term in enumerate(termDict):
        if term in doc:
            TFIDFVec[i] = doc[term]

    return TFIDFVec</code></pre>
</details>
</dd>
<dt id="plag_check.cosine_similarity"><code class="name flex">
<span>def <span class="ident">cosine_similarity</span></span>(<span>a, b)</span>
</code></dt>
<dd>
<div class="desc"><p>returns cosine similarity score of two vectors</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def cosine_similarity(a, b):
    &#34;&#34;&#34;returns cosine similarity score of two vectors&#34;&#34;&#34;
    cs = dot_product(a, b)/(norm(a) * norm(b))
    return cs</code></pre>
</details>
</dd>
<dt id="plag_check.dot_product"><code class="name flex">
<span>def <span class="ident">dot_product</span></span>(<span>a, b)</span>
</code></dt>
<dd>
<div class="desc"><p>returns dot product of two vectors</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def dot_product(a, b):
    &#34;&#34;&#34;returns dot product of two vectors&#34;&#34;&#34;
    dp = 0.0
    for i, j in zip(a, b):
        dp += i * j
    return dp</code></pre>
</details>
</dd>
<dt id="plag_check.lemmatize"><code class="name flex">
<span>def <span class="ident">lemmatize</span></span>(<span>data)</span>
</code></dt>
<dd>
<div class="desc"><p>lemmatizing the document</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def lemmatize(data):
    &#34;&#34;&#34;lemmatizing the document&#34;&#34;&#34;
    lemmatizer = WordNetLemmatizer()
    tokens = word_tokenize(str(data))
    new = &#34;&#34;
    for word in tokens:
        new = new + &#34; &#34; + lemmatizer.lemmatize(word)
    return new</code></pre>
</details>
</dd>
<dt id="plag_check.lowercase"><code class="name flex">
<span>def <span class="ident">lowercase</span></span>(<span>data)</span>
</code></dt>
<dd>
<div class="desc"><p>changes the case of all characters in the document to lowercase</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def lowercase(data):
    &#34;&#34;&#34;changes the case of all characters in the document to lowercase&#34;&#34;&#34;
    return np.char.lower(data)</code></pre>
</details>
</dd>
<dt id="plag_check.norm"><code class="name flex">
<span>def <span class="ident">norm</span></span>(<span>vec)</span>
</code></dt>
<dd>
<div class="desc"><p>returns the norm or magnitude of a vector</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def norm(vec):
    &#34;&#34;&#34;returns the norm or magnitude of a vector&#34;&#34;&#34;
    n = 0.0
    for i in vec:
        n += math.pow(i, 2)
    return math.sqrt(n)</code></pre>
</details>
</dd>
<dt id="plag_check.normalize"><code class="name flex">
<span>def <span class="ident">normalize</span></span>(<span>data)</span>
</code></dt>
<dd>
<div class="desc"><p>combining all the preprocessing functions in a suitable order</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def normalize(data):
    &#34;&#34;&#34;combining all the preprocessing functions in a suitable order&#34;&#34;&#34;
    data = lowercase(data)
    data = remove_punct(data)
    data = remove_apostrophes(data)
    data = remove_stopwords(data)
    data = num_to_words(data)
    data = lemmatize(data)
    data = stemming(data)
    data = remove_punct(data)
    data = num_to_words(data)
    data = lemmatize(data)
    data = stemming(data)
    data = remove_punct(data) #done again to remove hyphens produced by num2words
    data = remove_stopwords(data) #done agan to remove stopwords produced by num2words
    return data</code></pre>
</details>
</dd>
<dt id="plag_check.num_to_words"><code class="name flex">
<span>def <span class="ident">num_to_words</span></span>(<span>data)</span>
</code></dt>
<dd>
<div class="desc"><p>converting nunmbers to words in the document</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def num_to_words(data):
    &#34;&#34;&#34;converting nunmbers to words in the document&#34;&#34;&#34;
    tokens = word_tokenize(str(data))
    new = &#34;&#34;
    for word in tokens:
        try:
            word = num2words(int(w))
        except:
            a = 0
        new = new + &#34; &#34; + word
    new = np.char.replace(new, &#34;-&#34;, &#34; &#34;)
    return new</code></pre>
</details>
</dd>
<dt id="plag_check.remove_apostrophes"><code class="name flex">
<span>def <span class="ident">remove_apostrophes</span></span>(<span>data)</span>
</code></dt>
<dd>
<div class="desc"><p>removing apostrophes separately</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def remove_apostrophes(data):
    &#34;&#34;&#34;removing apostrophes separately&#34;&#34;&#34;
    data = np.char.replace(data, &#34;&#39;&#34;, &#34;&#34;)
    data = np.char.replace(data, &#34;창\x80\x98&#34;, &#34;&#34;) #removing unicode apostrophes
    data = np.char.replace(data, &#34;창\x80\x99&#34;, &#34;&#34;)
    return data</code></pre>
</details>
</dd>
<dt id="plag_check.remove_punct"><code class="name flex">
<span>def <span class="ident">remove_punct</span></span>(<span>data)</span>
</code></dt>
<dd>
<div class="desc"><p>removes all punctuation from the document</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def remove_punct(data):
    &#34;&#34;&#34;removes all punctuation from the document&#34;&#34;&#34;
    punct = &#34;!\&#34;#$%&amp;()*+-./:;&lt;=&gt;?@[\]^_`{|}~\n&#34;
    for i in range(len(punct)):
        data = np.char.replace(data, punct[i], &#39; &#39;)
        data = np.char.replace(data, &#34; &#34;, &#34; &#34;)
    data = np.char.replace(data, &#39;,&#39;, &#39;&#39;)
    return data</code></pre>
</details>
</dd>
<dt id="plag_check.remove_stopwords"><code class="name flex">
<span>def <span class="ident">remove_stopwords</span></span>(<span>data)</span>
</code></dt>
<dd>
<div class="desc"><p>removes stopwords from the document</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def remove_stopwords(data):
    &#34;&#34;&#34;removes stopwords from the document&#34;&#34;&#34;
    stop_words = stopwords.words(&#39;english&#39;)
    words = word_tokenize(str(data))
    new = &#34;&#34;
    for word in words:
        if word not in stop_words and len(word) &gt; 1:
            new = new + &#34; &#34; + word
    return new</code></pre>
</details>
</dd>
<dt id="plag_check.stemming"><code class="name flex">
<span>def <span class="ident">stemming</span></span>(<span>data)</span>
</code></dt>
<dd>
<div class="desc"><p>performing stemming on the tokens in the document</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def stemming(data):
    &#34;&#34;&#34;performing stemming on the tokens in the document&#34;&#34;&#34;
    stemmer = PorterStemmer()
    tokens = word_tokenize(str(data))
    new = &#34;&#34;
    for word in tokens:
        new = new + &#34; &#34; + stemmer.stem(word)
    return new</code></pre>
</details>
</dd>
</dl>
</section>
<section>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3><a href="#header-variables">Global variables</a></h3>
<ul class="">
<li><code><a title="plag_check.exec_start_time" href="#plag_check.exec_start_time">exec_start_time</a></code></li>
<li><code><a title="plag_check.exec_time" href="#plag_check.exec_time">exec_time</a></code></li>
<li><code><a title="plag_check.normalize_time" href="#plag_check.normalize_time">normalize_time</a></code></li>
<li><code><a title="plag_check.similarity_scores" href="#plag_check.similarity_scores">similarity_scores</a></code></li>
</ul>
</li>
<li><h3><a href="#header-functions">Functions</a></h3>
<ul class="two-column">
<li><code><a title="plag_check.calcCountDict" href="#plag_check.calcCountDict">calcCountDict</a></code></li>
<li><code><a title="plag_check.calcIDFDict" href="#plag_check.calcIDFDict">calcIDFDict</a></code></li>
<li><code><a title="plag_check.calcTFIDFDict" href="#plag_check.calcTFIDFDict">calcTFIDFDict</a></code></li>
<li><code><a title="plag_check.calcTFdict" href="#plag_check.calcTFdict">calcTFdict</a></code></li>
<li><code><a title="plag_check.calc_TF_IDF_Vector" href="#plag_check.calc_TF_IDF_Vector">calc_TF_IDF_Vector</a></code></li>
<li><code><a title="plag_check.cosine_similarity" href="#plag_check.cosine_similarity">cosine_similarity</a></code></li>
<li><code><a title="plag_check.dot_product" href="#plag_check.dot_product">dot_product</a></code></li>
<li><code><a title="plag_check.lemmatize" href="#plag_check.lemmatize">lemmatize</a></code></li>
<li><code><a title="plag_check.lowercase" href="#plag_check.lowercase">lowercase</a></code></li>
<li><code><a title="plag_check.norm" href="#plag_check.norm">norm</a></code></li>
<li><code><a title="plag_check.normalize" href="#plag_check.normalize">normalize</a></code></li>
<li><code><a title="plag_check.num_to_words" href="#plag_check.num_to_words">num_to_words</a></code></li>
<li><code><a title="plag_check.remove_apostrophes" href="#plag_check.remove_apostrophes">remove_apostrophes</a></code></li>
<li><code><a title="plag_check.remove_punct" href="#plag_check.remove_punct">remove_punct</a></code></li>
<li><code><a title="plag_check.remove_stopwords" href="#plag_check.remove_stopwords">remove_stopwords</a></code></li>
<li><code><a title="plag_check.stemming" href="#plag_check.stemming">stemming</a></code></li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc"><cite>pdoc</cite> 0.9.1</a>.</p>
</footer>
</body>
</html>