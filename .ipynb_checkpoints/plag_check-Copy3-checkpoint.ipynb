{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: num2words in c:\\users\\nade1l\\anaconda3_2\\lib\\site-packages (0.5.10)\n",
      "Requirement already satisfied: docopt>=0.6.2 in c:\\users\\nade1l\\anaconda3_2\\lib\\site-packages (from num2words) (0.6.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install num2words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\NaDe1L\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\NaDe1L\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\NaDe1L\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "\n",
    "from os.path import isfile\n",
    "from os.path import join\n",
    "\n",
    "import os\n",
    "from num2words import num2words\n",
    "import numpy as np\n",
    "import string\n",
    "import pandas as pd\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lowercase(data):\n",
    "    #changes the case of all characters in the document to lowercase\n",
    "    return np.char.lower(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(data):\n",
    "    #removes stopwords from the document\n",
    "    stop_words = stopwords.words('english')\n",
    "    words = word_tokenize(str(data))\n",
    "    new = \"\"\n",
    "    for word in words:\n",
    "        if word not in stop_words and len(word) > 1:\n",
    "            new = new + \" \" + word\n",
    "    return new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punct(data):\n",
    "    #removes all punctuation from the document\n",
    "    punct = \"!\\\"#$%&()*+-./:;<=>?@[\\]^_`{|}~\\n\"\n",
    "    for i in range(len(punct)):\n",
    "        data = np.char.replace(data, punct[i], ' ')\n",
    "        data = np.char.replace(data, \" \", \" \")\n",
    "    data = np.char.replace(data, ',', '')\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_apostrophes(data):\n",
    "    #removing apostrophes separately\n",
    "    data = np.char.replace(data, \"'\", \"\")\n",
    "    data = np.char.replace(data, \"â\\x80\\x98\", \"\") #removing unicode apostrophes\n",
    "    data = np.char.replace(data, \"â\\x80\\x99\", \"\")\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stemming(data):\n",
    "    #performing stemming on the tokens in the document\n",
    "    stemmer = PorterStemmer()\n",
    "    tokens = word_tokenize(str(data))\n",
    "    new = \"\"\n",
    "    for word in tokens:\n",
    "        new = new + \" \" + stemmer.stem(word)\n",
    "    return new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize(data):\n",
    "    #lemmatizing the document\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = word_tokenize(str(data))\n",
    "    new = \"\"\n",
    "    for word in tokens:\n",
    "        new = new + \" \" + lemmatizer.lemmatize(word)\n",
    "    return new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def num_to_words(data):\n",
    "    #converting nunmbers to words in the document\n",
    "    tokens = word_tokenize(str(data))\n",
    "    new = \"\"\n",
    "    for word in tokens:\n",
    "        try:\n",
    "            word = num2words(int(w))\n",
    "        except:\n",
    "            a = 0\n",
    "        new = new + \" \" + word\n",
    "    new = np.char.replace(new, \"-\", \" \")\n",
    "    return new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(data):\n",
    "    #combining all the above functions in a suitable order\n",
    "    data = lowercase(data)\n",
    "    data = remove_punct(data)\n",
    "    data = remove_apostrophes(data)\n",
    "    data = remove_stopwords(data)\n",
    "    data = num_to_words(data)\n",
    "    data = lemmatize(data)\n",
    "    data = stemming(data)\n",
    "    data = remove_punct(data)\n",
    "    data = num_to_words(data)\n",
    "    data = lemmatize(data)\n",
    "    data = stemming(data)\n",
    "    data = remove_punct(data) #done again to remove hyphens produced by num2words\n",
    "    data = remove_stopwords(data) #done agan to remove stopwords produced by num2words\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#computing tf dictionary\n",
    "\n",
    "def calcTFdict(doc):\n",
    "    \"\"\"Returns a term frequency dictionary for each document, with keys that are unique tokens in the document and values are the corresponding term frequencies\"\"\"\n",
    "    \n",
    "    TFDict = {}\n",
    "    \n",
    "    #counts number of appearances of term in document\n",
    "    for term in doc:\n",
    "        if term in TFDict.keys():\n",
    "            TFDict[term] +=1\n",
    "        else:\n",
    "            TFDict[term] = 1\n",
    "            \n",
    "    #Computing tf for each term\n",
    "    for key in TFDict:\n",
    "        TFDict[key] = TFDict[key]/len(doc)\n",
    "    \n",
    "    return TFDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcCountDict(TFdict):\n",
    "    \"\"\"Returns dictionary with keys as all the unique terms in corpus and values is the number of documents in which each term appears\"\"\"\n",
    "    \n",
    "    countDict = {}\n",
    "    \n",
    "    for doc in TFdict:\n",
    "        for term in doc:\n",
    "            if term in countDict:\n",
    "                countDict[term] +=1\n",
    "            else:\n",
    "                countDict[term] = 1\n",
    "                \n",
    "    return countDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#computing idf dictionary\n",
    "\n",
    "def calcIDFDict(countDict, numfiles):\n",
    "    \"\"\"Returns dictionary whose keys are all unique words in dataset and values are corresponding Inverted Document Frequencies\"\"\"\n",
    "    \n",
    "    IDFDict = {}\n",
    "    for term in countDict:\n",
    "        IDFDict[term] = math.log(numfiles / countDict[term])\n",
    "    \n",
    "    return IDFDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculating TF-IDF dictionary\n",
    "def calcTFIDFDict(TFDict, IDFDict):\n",
    "    \"\"\"Returns dictionary whose keys are all unique terms in the document and values are corresponding TF-IDF value\"\"\"\n",
    "    \n",
    "    TFIDFDict = {}\n",
    "    \n",
    "    #for each term in the document, multiply the tf and idf values\n",
    "    \n",
    "    for term in TFDict:\n",
    "        TFIDFDict[term] = TFDict[term] * IDFDict[term]\n",
    "\n",
    "    return TFIDFDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating TF-IDF vector (for calculating cosine similarity)\n",
    "\n",
    "def calc_TF_IDF_Vector(doc, termDict):\n",
    "    TFIDFVec = [0.0] * len(termDict)\n",
    "    \n",
    "    #for each unique term, if it is in the document, store the TF-IDF value\n",
    "    for i, term in enumerate(termDict):\n",
    "        if term in doc:\n",
    "            TFIDFVec[i] = doc[term]\n",
    "        \n",
    "    return TFIDFVec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def cosine_similarity(a, b):\n",
    "    cs = np.dot(a,b)/(np.linalg.norm(a)*np.linalg.norm(b))\n",
    "    return cs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dot_product(a, b):\n",
    "    #returns dot product of two vectors\n",
    "    dp = 0.0\n",
    "    for i, j in zip(a, b):\n",
    "        dp += i * j\n",
    "    return dp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def norm(vec):\n",
    "    #returns the norm or magnitude of a vector\n",
    "    n = 0.0\n",
    "    for i in vec:\n",
    "        n += math.pow(i, 2)\n",
    "    return math.sqrt(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(a, b):\n",
    "    #returns cosine similarity score of two vectors\n",
    "    cs = dot_product(a, b)/(norm(a) * norm(b))\n",
    "    return cs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_trg = []\n",
    "path_trg = \"./texts/\" #directory in which training set is located\n",
    "path_test = \"./test/\"\n",
    "test_file = input(\"Enter file name: \") #g4pC_taska.txt\n",
    "trg_files = [document for document in os.listdir(path_trg) if document.endswith('.txt')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "numfiles_trg = 0 #number of files in the training directory\n",
    "for file in trg_files:\n",
    "    file.encode('utf8').strip() #encodes each of the files into utf-8\n",
    "    fh = open(os.path.join(path_trg, file), 'r', encoding = \"utf-8\")\n",
    "    file_content = fh.read()\n",
    "    numfiles_trg = numfiles_trg + 1\n",
    "\n",
    "    normalized_trg.append(word_tokenize(str(normalize(file_content)))) #performing normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_file.encode('utf8').strip()\n",
    "test_file_handle = open(os.path.join(path_test, test_file), 'r', encoding = \"utf-8\")\n",
    "test_file_content = test_file_handle.read()\n",
    "normalized_test = [(word_tokenize(str(normalize(test_file_content))))] #performing normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_corpus = normalized_trg + normalized_test\n",
    "test_doc_index = len(normalized_corpus) - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "TFdict_trg = [] #term frequency dictionary of the training set\n",
    "for i in range(len(normalized_trg)):\n",
    "    d = calcTFdict(normalized_trg[i])\n",
    "    TFdict_trg.append(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "countDict_trg = calcCountDict(TFdict_trg)\n",
    "#calculating the number of documents in which each term appears"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "IDFDict_trg = calcIDFDict(countDict_trg, numfiles_trg)\n",
    "#calculating the IDF dictionary of the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "TFIDFDict_trg = [calcTFIDFDict(doc, IDFDict_trg) for doc in TFdict_trg]\n",
    "#calculating the TF-IDF dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "termDict_trg = sorted(countDict_trg.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf_vector_trg = [calc_TF_IDF_Vector(doc, termDict_trg) for doc in TFIDFDict_trg]\n",
    "#vectorizing the TF-IDF dictionary for the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim = cosine_similarity(tf_idf_vector_trg[1], tf_idf_vector_trg[19])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.63878578153549"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOW CHECKING SIMILARITY AGAINST TEST SET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter file name: g4pC_taska.txt\n"
     ]
    }
   ],
   "source": [
    "test_file_name = input(\"Enter file name: \") #g4pC_taska.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_test = \"./test\"\n",
    "test_file_name.encode('utf8').strip() #encodes test file into UTF-8\n",
    "test_file_handle = open(os.path.join(path_test, test_file_name), 'r', encoding = \"utf-8\")\n",
    "test_file_content = test_file_handle.read()\n",
    "\n",
    "normalized_test = [(word_tokenize(str(normalize(test_file_content))))] #performing normalization\n",
    "numfiles_test = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['object',\n",
       "  'orient',\n",
       "  'program',\n",
       "  'inherit',\n",
       "  'way',\n",
       "  'form',\n",
       "  'new',\n",
       "  'class',\n",
       "  'instanc',\n",
       "  'call',\n",
       "  'object',\n",
       "  'use',\n",
       "  'class',\n",
       "  'alreadi',\n",
       "  'defin',\n",
       "  'inherit',\n",
       "  'concept',\n",
       "  'invent',\n",
       "  '1967',\n",
       "  'simula',\n",
       "  'inherit',\n",
       "  'provid',\n",
       "  'support',\n",
       "  'repres',\n",
       "  'categor',\n",
       "  'comput',\n",
       "  'languag',\n",
       "  'categor',\n",
       "  'power',\n",
       "  'mechan',\n",
       "  'number',\n",
       "  'inform',\n",
       "  'process',\n",
       "  'crucial',\n",
       "  'human',\n",
       "  'learn',\n",
       "  'mean',\n",
       "  'gener',\n",
       "  'cognit',\n",
       "  'economi',\n",
       "  'le',\n",
       "  'inform',\n",
       "  'need',\n",
       "  'store',\n",
       "  'specif',\n",
       "  'entiti',\n",
       "  'particular',\n",
       "  'new',\n",
       "  'class',\n",
       "  'known',\n",
       "  'deriv',\n",
       "  'class',\n",
       "  'take',\n",
       "  'inherit',\n",
       "  'attribut',\n",
       "  'behavior',\n",
       "  'pre',\n",
       "  'exist',\n",
       "  'class',\n",
       "  'refer',\n",
       "  'base',\n",
       "  'class',\n",
       "  'ancestor',\n",
       "  'class',\n",
       "  'intend',\n",
       "  'help',\n",
       "  'reu',\n",
       "  'exist',\n",
       "  'code',\n",
       "  'littl',\n",
       "  'modif',\n",
       "  'inherit',\n",
       "  'also',\n",
       "  'sometim',\n",
       "  'call',\n",
       "  'gener',\n",
       "  'relationship',\n",
       "  'repr',\n",
       "  'hierarchi',\n",
       "  'class',\n",
       "  'object',\n",
       "  'instanc',\n",
       "  'fruit',\n",
       "  'gener',\n",
       "  'appl',\n",
       "  'orang',\n",
       "  'mango',\n",
       "  'mani',\n",
       "  'one',\n",
       "  'consid',\n",
       "  'fruit',\n",
       "  'abstract',\n",
       "  'appl',\n",
       "  'orang',\n",
       "  'etc',\n",
       "  'conver',\n",
       "  'sinc',\n",
       "  'appl',\n",
       "  'fruit',\n",
       "  'appl',\n",
       "  'fruit',\n",
       "  'appl',\n",
       "  'may',\n",
       "  'natur',\n",
       "  'inherit',\n",
       "  'properti',\n",
       "  'common',\n",
       "  'fruit',\n",
       "  'fleshi',\n",
       "  'contain',\n",
       "  'seed',\n",
       "  'plant',\n",
       "  'advantag',\n",
       "  'inherit',\n",
       "  'modul',\n",
       "  'suffici',\n",
       "  'similar',\n",
       "  'interfac',\n",
       "  'share',\n",
       "  'lot',\n",
       "  'code',\n",
       "  'reduc',\n",
       "  'complex',\n",
       "  'program',\n",
       "  'inherit',\n",
       "  'therefor',\n",
       "  'anoth',\n",
       "  'view',\n",
       "  'dual',\n",
       "  'call',\n",
       "  'polymorph',\n",
       "  'describ',\n",
       "  'mani',\n",
       "  'piec',\n",
       "  'code',\n",
       "  'control',\n",
       "  'share',\n",
       "  'control',\n",
       "  'code',\n",
       "  'inherit',\n",
       "  'typic',\n",
       "  'accomplish',\n",
       "  'either',\n",
       "  'overrid',\n",
       "  'replac',\n",
       "  'one',\n",
       "  'method',\n",
       "  'expo',\n",
       "  'ancestor',\n",
       "  'ad',\n",
       "  'new',\n",
       "  'method',\n",
       "  'expo',\n",
       "  'ancestor',\n",
       "  'complex',\n",
       "  'inherit',\n",
       "  'inherit',\n",
       "  'use',\n",
       "  'within',\n",
       "  'design',\n",
       "  'suffici',\n",
       "  'matur',\n",
       "  'may',\n",
       "  'lead',\n",
       "  'yo',\n",
       "  'yo',\n",
       "  'problem']]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalized_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "TFdict_test = [] #term frequency dictionary of the test document\n",
    "for i in range(len(normalized_test)):\n",
    "    d = calcTFdict(normalized_test[i])\n",
    "    TFdict_test.append(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'object': 0.017964071856287425,\n",
       " 'orient': 0.005988023952095809,\n",
       " 'program': 0.011976047904191617,\n",
       " 'inherit': 0.0658682634730539,\n",
       " 'way': 0.005988023952095809,\n",
       " 'form': 0.005988023952095809,\n",
       " 'new': 0.017964071856287425,\n",
       " 'class': 0.04790419161676647,\n",
       " 'instanc': 0.011976047904191617,\n",
       " 'call': 0.017964071856287425,\n",
       " 'use': 0.011976047904191617,\n",
       " 'alreadi': 0.005988023952095809,\n",
       " 'defin': 0.005988023952095809,\n",
       " 'concept': 0.005988023952095809,\n",
       " 'invent': 0.005988023952095809,\n",
       " '1967': 0.005988023952095809,\n",
       " 'simula': 0.005988023952095809,\n",
       " 'provid': 0.005988023952095809,\n",
       " 'support': 0.005988023952095809,\n",
       " 'repres': 0.005988023952095809,\n",
       " 'categor': 0.011976047904191617,\n",
       " 'comput': 0.005988023952095809,\n",
       " 'languag': 0.005988023952095809,\n",
       " 'power': 0.005988023952095809,\n",
       " 'mechan': 0.005988023952095809,\n",
       " 'number': 0.005988023952095809,\n",
       " 'inform': 0.011976047904191617,\n",
       " 'process': 0.005988023952095809,\n",
       " 'crucial': 0.005988023952095809,\n",
       " 'human': 0.005988023952095809,\n",
       " 'learn': 0.005988023952095809,\n",
       " 'mean': 0.005988023952095809,\n",
       " 'gener': 0.017964071856287425,\n",
       " 'cognit': 0.005988023952095809,\n",
       " 'economi': 0.005988023952095809,\n",
       " 'le': 0.005988023952095809,\n",
       " 'need': 0.005988023952095809,\n",
       " 'store': 0.005988023952095809,\n",
       " 'specif': 0.005988023952095809,\n",
       " 'entiti': 0.005988023952095809,\n",
       " 'particular': 0.005988023952095809,\n",
       " 'known': 0.005988023952095809,\n",
       " 'deriv': 0.005988023952095809,\n",
       " 'take': 0.005988023952095809,\n",
       " 'attribut': 0.005988023952095809,\n",
       " 'behavior': 0.005988023952095809,\n",
       " 'pre': 0.005988023952095809,\n",
       " 'exist': 0.011976047904191617,\n",
       " 'refer': 0.005988023952095809,\n",
       " 'base': 0.005988023952095809,\n",
       " 'ancestor': 0.017964071856287425,\n",
       " 'intend': 0.005988023952095809,\n",
       " 'help': 0.005988023952095809,\n",
       " 'reu': 0.005988023952095809,\n",
       " 'code': 0.023952095808383235,\n",
       " 'littl': 0.005988023952095809,\n",
       " 'modif': 0.005988023952095809,\n",
       " 'also': 0.005988023952095809,\n",
       " 'sometim': 0.005988023952095809,\n",
       " 'relationship': 0.005988023952095809,\n",
       " 'repr': 0.005988023952095809,\n",
       " 'hierarchi': 0.005988023952095809,\n",
       " 'fruit': 0.029940119760479042,\n",
       " 'appl': 0.029940119760479042,\n",
       " 'orang': 0.011976047904191617,\n",
       " 'mango': 0.005988023952095809,\n",
       " 'mani': 0.011976047904191617,\n",
       " 'one': 0.011976047904191617,\n",
       " 'consid': 0.005988023952095809,\n",
       " 'abstract': 0.005988023952095809,\n",
       " 'etc': 0.005988023952095809,\n",
       " 'conver': 0.005988023952095809,\n",
       " 'sinc': 0.005988023952095809,\n",
       " 'may': 0.011976047904191617,\n",
       " 'natur': 0.005988023952095809,\n",
       " 'properti': 0.005988023952095809,\n",
       " 'common': 0.005988023952095809,\n",
       " 'fleshi': 0.005988023952095809,\n",
       " 'contain': 0.005988023952095809,\n",
       " 'seed': 0.005988023952095809,\n",
       " 'plant': 0.005988023952095809,\n",
       " 'advantag': 0.005988023952095809,\n",
       " 'modul': 0.005988023952095809,\n",
       " 'suffici': 0.011976047904191617,\n",
       " 'similar': 0.005988023952095809,\n",
       " 'interfac': 0.005988023952095809,\n",
       " 'share': 0.011976047904191617,\n",
       " 'lot': 0.005988023952095809,\n",
       " 'reduc': 0.005988023952095809,\n",
       " 'complex': 0.011976047904191617,\n",
       " 'therefor': 0.005988023952095809,\n",
       " 'anoth': 0.005988023952095809,\n",
       " 'view': 0.005988023952095809,\n",
       " 'dual': 0.005988023952095809,\n",
       " 'polymorph': 0.005988023952095809,\n",
       " 'describ': 0.005988023952095809,\n",
       " 'piec': 0.005988023952095809,\n",
       " 'control': 0.011976047904191617,\n",
       " 'typic': 0.005988023952095809,\n",
       " 'accomplish': 0.005988023952095809,\n",
       " 'either': 0.005988023952095809,\n",
       " 'overrid': 0.005988023952095809,\n",
       " 'replac': 0.005988023952095809,\n",
       " 'method': 0.011976047904191617,\n",
       " 'expo': 0.011976047904191617,\n",
       " 'ad': 0.005988023952095809,\n",
       " 'within': 0.005988023952095809,\n",
       " 'design': 0.005988023952095809,\n",
       " 'matur': 0.005988023952095809,\n",
       " 'lead': 0.005988023952095809,\n",
       " 'yo': 0.011976047904191617,\n",
       " 'problem': 0.005988023952095809}"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TFdict_test[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "countDict_test = calcCountDict(TFdict_test)\n",
    "#calculating the number of documents in which each term appears"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'object': 1,\n",
       " 'orient': 1,\n",
       " 'program': 1,\n",
       " 'inherit': 1,\n",
       " 'way': 1,\n",
       " 'form': 1,\n",
       " 'new': 1,\n",
       " 'class': 1,\n",
       " 'instanc': 1,\n",
       " 'call': 1,\n",
       " 'use': 1,\n",
       " 'alreadi': 1,\n",
       " 'defin': 1,\n",
       " 'concept': 1,\n",
       " 'invent': 1,\n",
       " '1967': 1,\n",
       " 'simula': 1,\n",
       " 'provid': 1,\n",
       " 'support': 1,\n",
       " 'repres': 1,\n",
       " 'categor': 1,\n",
       " 'comput': 1,\n",
       " 'languag': 1,\n",
       " 'power': 1,\n",
       " 'mechan': 1,\n",
       " 'number': 1,\n",
       " 'inform': 1,\n",
       " 'process': 1,\n",
       " 'crucial': 1,\n",
       " 'human': 1,\n",
       " 'learn': 1,\n",
       " 'mean': 1,\n",
       " 'gener': 1,\n",
       " 'cognit': 1,\n",
       " 'economi': 1,\n",
       " 'le': 1,\n",
       " 'need': 1,\n",
       " 'store': 1,\n",
       " 'specif': 1,\n",
       " 'entiti': 1,\n",
       " 'particular': 1,\n",
       " 'known': 1,\n",
       " 'deriv': 1,\n",
       " 'take': 1,\n",
       " 'attribut': 1,\n",
       " 'behavior': 1,\n",
       " 'pre': 1,\n",
       " 'exist': 1,\n",
       " 'refer': 1,\n",
       " 'base': 1,\n",
       " 'ancestor': 1,\n",
       " 'intend': 1,\n",
       " 'help': 1,\n",
       " 'reu': 1,\n",
       " 'code': 1,\n",
       " 'littl': 1,\n",
       " 'modif': 1,\n",
       " 'also': 1,\n",
       " 'sometim': 1,\n",
       " 'relationship': 1,\n",
       " 'repr': 1,\n",
       " 'hierarchi': 1,\n",
       " 'fruit': 1,\n",
       " 'appl': 1,\n",
       " 'orang': 1,\n",
       " 'mango': 1,\n",
       " 'mani': 1,\n",
       " 'one': 1,\n",
       " 'consid': 1,\n",
       " 'abstract': 1,\n",
       " 'etc': 1,\n",
       " 'conver': 1,\n",
       " 'sinc': 1,\n",
       " 'may': 1,\n",
       " 'natur': 1,\n",
       " 'properti': 1,\n",
       " 'common': 1,\n",
       " 'fleshi': 1,\n",
       " 'contain': 1,\n",
       " 'seed': 1,\n",
       " 'plant': 1,\n",
       " 'advantag': 1,\n",
       " 'modul': 1,\n",
       " 'suffici': 1,\n",
       " 'similar': 1,\n",
       " 'interfac': 1,\n",
       " 'share': 1,\n",
       " 'lot': 1,\n",
       " 'reduc': 1,\n",
       " 'complex': 1,\n",
       " 'therefor': 1,\n",
       " 'anoth': 1,\n",
       " 'view': 1,\n",
       " 'dual': 1,\n",
       " 'polymorph': 1,\n",
       " 'describ': 1,\n",
       " 'piec': 1,\n",
       " 'control': 1,\n",
       " 'typic': 1,\n",
       " 'accomplish': 1,\n",
       " 'either': 1,\n",
       " 'overrid': 1,\n",
       " 'replac': 1,\n",
       " 'method': 1,\n",
       " 'expo': 1,\n",
       " 'ad': 1,\n",
       " 'within': 1,\n",
       " 'design': 1,\n",
       " 'matur': 1,\n",
       " 'lead': 1,\n",
       " 'yo': 1,\n",
       " 'problem': 1}"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "countDict_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "IDFDict_test = calcIDFDict(countDict_test, numfiles_test)\n",
    "#calculating the IDF dictionary of the test document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'object': 0.0,\n",
       " 'orient': 0.0,\n",
       " 'program': 0.0,\n",
       " 'inherit': 0.0,\n",
       " 'way': 0.0,\n",
       " 'form': 0.0,\n",
       " 'new': 0.0,\n",
       " 'class': 0.0,\n",
       " 'instanc': 0.0,\n",
       " 'call': 0.0,\n",
       " 'use': 0.0,\n",
       " 'alreadi': 0.0,\n",
       " 'defin': 0.0,\n",
       " 'concept': 0.0,\n",
       " 'invent': 0.0,\n",
       " '1967': 0.0,\n",
       " 'simula': 0.0,\n",
       " 'provid': 0.0,\n",
       " 'support': 0.0,\n",
       " 'repres': 0.0,\n",
       " 'categor': 0.0,\n",
       " 'comput': 0.0,\n",
       " 'languag': 0.0,\n",
       " 'power': 0.0,\n",
       " 'mechan': 0.0,\n",
       " 'number': 0.0,\n",
       " 'inform': 0.0,\n",
       " 'process': 0.0,\n",
       " 'crucial': 0.0,\n",
       " 'human': 0.0,\n",
       " 'learn': 0.0,\n",
       " 'mean': 0.0,\n",
       " 'gener': 0.0,\n",
       " 'cognit': 0.0,\n",
       " 'economi': 0.0,\n",
       " 'le': 0.0,\n",
       " 'need': 0.0,\n",
       " 'store': 0.0,\n",
       " 'specif': 0.0,\n",
       " 'entiti': 0.0,\n",
       " 'particular': 0.0,\n",
       " 'known': 0.0,\n",
       " 'deriv': 0.0,\n",
       " 'take': 0.0,\n",
       " 'attribut': 0.0,\n",
       " 'behavior': 0.0,\n",
       " 'pre': 0.0,\n",
       " 'exist': 0.0,\n",
       " 'refer': 0.0,\n",
       " 'base': 0.0,\n",
       " 'ancestor': 0.0,\n",
       " 'intend': 0.0,\n",
       " 'help': 0.0,\n",
       " 'reu': 0.0,\n",
       " 'code': 0.0,\n",
       " 'littl': 0.0,\n",
       " 'modif': 0.0,\n",
       " 'also': 0.0,\n",
       " 'sometim': 0.0,\n",
       " 'relationship': 0.0,\n",
       " 'repr': 0.0,\n",
       " 'hierarchi': 0.0,\n",
       " 'fruit': 0.0,\n",
       " 'appl': 0.0,\n",
       " 'orang': 0.0,\n",
       " 'mango': 0.0,\n",
       " 'mani': 0.0,\n",
       " 'one': 0.0,\n",
       " 'consid': 0.0,\n",
       " 'abstract': 0.0,\n",
       " 'etc': 0.0,\n",
       " 'conver': 0.0,\n",
       " 'sinc': 0.0,\n",
       " 'may': 0.0,\n",
       " 'natur': 0.0,\n",
       " 'properti': 0.0,\n",
       " 'common': 0.0,\n",
       " 'fleshi': 0.0,\n",
       " 'contain': 0.0,\n",
       " 'seed': 0.0,\n",
       " 'plant': 0.0,\n",
       " 'advantag': 0.0,\n",
       " 'modul': 0.0,\n",
       " 'suffici': 0.0,\n",
       " 'similar': 0.0,\n",
       " 'interfac': 0.0,\n",
       " 'share': 0.0,\n",
       " 'lot': 0.0,\n",
       " 'reduc': 0.0,\n",
       " 'complex': 0.0,\n",
       " 'therefor': 0.0,\n",
       " 'anoth': 0.0,\n",
       " 'view': 0.0,\n",
       " 'dual': 0.0,\n",
       " 'polymorph': 0.0,\n",
       " 'describ': 0.0,\n",
       " 'piec': 0.0,\n",
       " 'control': 0.0,\n",
       " 'typic': 0.0,\n",
       " 'accomplish': 0.0,\n",
       " 'either': 0.0,\n",
       " 'overrid': 0.0,\n",
       " 'replac': 0.0,\n",
       " 'method': 0.0,\n",
       " 'expo': 0.0,\n",
       " 'ad': 0.0,\n",
       " 'within': 0.0,\n",
       " 'design': 0.0,\n",
       " 'matur': 0.0,\n",
       " 'lead': 0.0,\n",
       " 'yo': 0.0,\n",
       " 'problem': 0.0}"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "IDFDict_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "TFIDFDict_test = [calcTFIDFDict(doc, IDFDict_test) for doc in TFdict_test]\n",
    "#calculating the TF-IDF dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'object': 0.0,\n",
       "  'orient': 0.0,\n",
       "  'program': 0.0,\n",
       "  'inherit': 0.0,\n",
       "  'way': 0.0,\n",
       "  'form': 0.0,\n",
       "  'new': 0.0,\n",
       "  'class': 0.0,\n",
       "  'instanc': 0.0,\n",
       "  'call': 0.0,\n",
       "  'use': 0.0,\n",
       "  'alreadi': 0.0,\n",
       "  'defin': 0.0,\n",
       "  'concept': 0.0,\n",
       "  'invent': 0.0,\n",
       "  '1967': 0.0,\n",
       "  'simula': 0.0,\n",
       "  'provid': 0.0,\n",
       "  'support': 0.0,\n",
       "  'repres': 0.0,\n",
       "  'categor': 0.0,\n",
       "  'comput': 0.0,\n",
       "  'languag': 0.0,\n",
       "  'power': 0.0,\n",
       "  'mechan': 0.0,\n",
       "  'number': 0.0,\n",
       "  'inform': 0.0,\n",
       "  'process': 0.0,\n",
       "  'crucial': 0.0,\n",
       "  'human': 0.0,\n",
       "  'learn': 0.0,\n",
       "  'mean': 0.0,\n",
       "  'gener': 0.0,\n",
       "  'cognit': 0.0,\n",
       "  'economi': 0.0,\n",
       "  'le': 0.0,\n",
       "  'need': 0.0,\n",
       "  'store': 0.0,\n",
       "  'specif': 0.0,\n",
       "  'entiti': 0.0,\n",
       "  'particular': 0.0,\n",
       "  'known': 0.0,\n",
       "  'deriv': 0.0,\n",
       "  'take': 0.0,\n",
       "  'attribut': 0.0,\n",
       "  'behavior': 0.0,\n",
       "  'pre': 0.0,\n",
       "  'exist': 0.0,\n",
       "  'refer': 0.0,\n",
       "  'base': 0.0,\n",
       "  'ancestor': 0.0,\n",
       "  'intend': 0.0,\n",
       "  'help': 0.0,\n",
       "  'reu': 0.0,\n",
       "  'code': 0.0,\n",
       "  'littl': 0.0,\n",
       "  'modif': 0.0,\n",
       "  'also': 0.0,\n",
       "  'sometim': 0.0,\n",
       "  'relationship': 0.0,\n",
       "  'repr': 0.0,\n",
       "  'hierarchi': 0.0,\n",
       "  'fruit': 0.0,\n",
       "  'appl': 0.0,\n",
       "  'orang': 0.0,\n",
       "  'mango': 0.0,\n",
       "  'mani': 0.0,\n",
       "  'one': 0.0,\n",
       "  'consid': 0.0,\n",
       "  'abstract': 0.0,\n",
       "  'etc': 0.0,\n",
       "  'conver': 0.0,\n",
       "  'sinc': 0.0,\n",
       "  'may': 0.0,\n",
       "  'natur': 0.0,\n",
       "  'properti': 0.0,\n",
       "  'common': 0.0,\n",
       "  'fleshi': 0.0,\n",
       "  'contain': 0.0,\n",
       "  'seed': 0.0,\n",
       "  'plant': 0.0,\n",
       "  'advantag': 0.0,\n",
       "  'modul': 0.0,\n",
       "  'suffici': 0.0,\n",
       "  'similar': 0.0,\n",
       "  'interfac': 0.0,\n",
       "  'share': 0.0,\n",
       "  'lot': 0.0,\n",
       "  'reduc': 0.0,\n",
       "  'complex': 0.0,\n",
       "  'therefor': 0.0,\n",
       "  'anoth': 0.0,\n",
       "  'view': 0.0,\n",
       "  'dual': 0.0,\n",
       "  'polymorph': 0.0,\n",
       "  'describ': 0.0,\n",
       "  'piec': 0.0,\n",
       "  'control': 0.0,\n",
       "  'typic': 0.0,\n",
       "  'accomplish': 0.0,\n",
       "  'either': 0.0,\n",
       "  'overrid': 0.0,\n",
       "  'replac': 0.0,\n",
       "  'method': 0.0,\n",
       "  'expo': 0.0,\n",
       "  'ad': 0.0,\n",
       "  'within': 0.0,\n",
       "  'design': 0.0,\n",
       "  'matur': 0.0,\n",
       "  'lead': 0.0,\n",
       "  'yo': 0.0,\n",
       "  'problem': 0.0}]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TFIDFDict_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "termDict_test = sorted(countDict_test.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf_vector_test = [calc_TF_IDF_Vector(doc, termDict_test) for doc in TFIDFDict_test]\n",
    "#vectorizing the TF-IDF dictionary for the test document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_idf_vector_test[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "ename": "ZeroDivisionError",
     "evalue": "float division by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-52-2b29c7599dd7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0msimilarity_scores\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf_idf_vector_trg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[0mcs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcosine_similarity\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf_idf_vector_test\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtf_idf_vector_trg\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m     \u001b[0msimilarity_scores\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-30-0d032a196ee0>\u001b[0m in \u001b[0;36mcosine_similarity\u001b[1;34m(a, b)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mcosine_similarity\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[1;31m#returns cosine similarity score of two vectors\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0mcs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdot_product\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnorm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mnorm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mcs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mZeroDivisionError\u001b[0m: float division by zero"
     ]
    }
   ],
   "source": [
    "#Calculating cosine similarity of test document with respect to all other documents in the training set\n",
    "similarity_scores = []\n",
    "for i in range(len(tf_idf_vector_trg)):\n",
    "    cs = cosine_similarity(tf_idf_vector_test[0], tf_idf_vector_trg[i])\n",
    "    similarity_scores.append(cs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity_scores"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
