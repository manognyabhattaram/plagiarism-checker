{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: num2words in c:\\users\\nade1l\\anaconda3_2\\lib\\site-packages (0.5.10)\n",
      "Requirement already satisfied: docopt>=0.6.2 in c:\\users\\nade1l\\anaconda3_2\\lib\\site-packages (from num2words) (0.6.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install num2words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\NaDe1L\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\NaDe1L\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\NaDe1L\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "\n",
    "from os.path import isfile\n",
    "from os.path import join\n",
    "\n",
    "import os\n",
    "from num2words import num2words\n",
    "import numpy as np\n",
    "import string\n",
    "import pandas as pd\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lowercase(data):\n",
    "    #changes the case of all characters in the document to lowercase\n",
    "    return np.char.lower(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(data):\n",
    "    #removes stopwords from the document\n",
    "    stop_words = stopwords.words('english')\n",
    "    words = word_tokenize(str(data))\n",
    "    new = \"\"\n",
    "    for word in words:\n",
    "        if word not in stop_words and len(word) > 1:\n",
    "            new = new + \" \" + word\n",
    "    return new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punct(data):\n",
    "    #removes all punctuation from the document\n",
    "    punct = \"!\\\"#$%&()*+-./:;<=>?@[\\]^_`{|}~\\n\"\n",
    "    for i in range(len(punct)):\n",
    "        data = np.char.replace(data, punct[i], ' ')\n",
    "        data = np.char.replace(data, \" \", \" \")\n",
    "    data = np.char.replace(data, ',', '')\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_apostrophes(data):\n",
    "    #removing apostrophes separately\n",
    "    data = np.char.replace(data, \"'\", \"\")\n",
    "    data = np.char.replace(data, \"â\\x80\\x98\", \"\") #removing unicode apostrophes\n",
    "    data = np.char.replace(data, \"â\\x80\\x99\", \"\")\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stemming(data):\n",
    "    #performing stemming on the tokens in the document\n",
    "    stemmer = PorterStemmer()\n",
    "    tokens = word_tokenize(str(data))\n",
    "    new = \"\"\n",
    "    for word in tokens:\n",
    "        new = new + \" \" + stemmer.stem(word)\n",
    "    return new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize(data):\n",
    "    #lemmatizing the document\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = word_tokenize(str(data))\n",
    "    new = \"\"\n",
    "    for word in tokens:\n",
    "        new = new + \" \" + lemmatizer.lemmatize(word)\n",
    "    return new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def num_to_words(data):\n",
    "    #converting nunmbers to words in the document\n",
    "    tokens = word_tokenize(str(data))\n",
    "    new = \"\"\n",
    "    for word in tokens:\n",
    "        try:\n",
    "            word = num2words(int(w))\n",
    "        except:\n",
    "            a = 0\n",
    "        new = new + \" \" + word\n",
    "    new = np.char.replace(new, \"-\", \" \")\n",
    "    return new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(data):\n",
    "    #combining all the above functions in a suitable order\n",
    "    data = lowercase(data)\n",
    "    data = remove_punct(data)\n",
    "    data = remove_apostrophes(data)\n",
    "    data = remove_stopwords(data)\n",
    "    data = num_to_words(data)\n",
    "    data = lemmatize(data)\n",
    "    data = stemming(data)\n",
    "    data = remove_punct(data)\n",
    "    data = num_to_words(data)\n",
    "    data = lemmatize(data)\n",
    "    data = stemming(data)\n",
    "    data = remove_punct(data) #done again to remove hyphens produced by num2words\n",
    "    data = remove_stopwords(data) #done agan to remove stopwords produced by num2words\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#computing tf dictionary\n",
    "\n",
    "def calcTFdict(doc):\n",
    "    \"\"\"Returns a term frequency dictionary for each document, with keys that are unique tokens in the document and values are the corresponding term frequencies\"\"\"\n",
    "    \n",
    "    TFDict = {}\n",
    "    \n",
    "    #counts number of appearances of term in document\n",
    "    for term in doc:\n",
    "        if term in TFDict.keys():\n",
    "            TFDict[term] +=1\n",
    "        else:\n",
    "            TFDict[term] = 1\n",
    "            \n",
    "    #Computing tf for each term\n",
    "    for key in TFDict:\n",
    "        TFDict[key] = TFDict[key]/len(doc)\n",
    "    \n",
    "    return TFDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcCountDict():\n",
    "    \"\"\"Returns dictionary with keys as all the unique terms in corpus and values is the number of documents in which each term appears\"\"\"\n",
    "    \n",
    "    countDict = {}\n",
    "    \n",
    "    for doc in TFdict:\n",
    "        for term in doc:\n",
    "            if term in countDict:\n",
    "                countDict[term] +=1\n",
    "            else:\n",
    "                countDict[term] = 1\n",
    "                \n",
    "    return countDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#computing idf dictionary\n",
    "\n",
    "def calcIDFDict():\n",
    "    \"\"\"Returns dictionary whose keys are all unique words in dataset and values are corresponding Inverted Document Frequencies\"\"\"\n",
    "    \n",
    "    IDFDict = {}\n",
    "    for term in countDict:\n",
    "        IDFDict[term] = math.log(numfiles / countDict[term])\n",
    "    \n",
    "    return IDFDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculating TF-IDF dictionary\n",
    "def calcTFIDFDict(TFDict):\n",
    "    \"\"\"Returns dictionary whose keys are all unique terms in the document and values are corresponding TF-IDF value\"\"\"\n",
    "    \n",
    "    TFIDFDict = {}\n",
    "    \n",
    "    #for each term in the document, multiply the tf and idf values\n",
    "    \n",
    "    for term in TFDict:\n",
    "        TFIDFDict[term] = TFDict[term] * IDFDict[term]\n",
    "\n",
    "    return TFIDFDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating TF-IDF vector (for calculating cosine similarity)\n",
    "\n",
    "def calc_TF_IDF_Vector(doc):\n",
    "    TFIDFVec = [0.0] * len(termDict)\n",
    "    \n",
    "    #for each unique term, if it is in the document, store the TF-IDF value\n",
    "    for i, term in enumerate(termDict):\n",
    "        if term in doc:\n",
    "            TFIDFVec[i] = doc[term]\n",
    "        \n",
    "    return TFIDFVec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(a, b):\n",
    "    cs = np.dot(a,b)/(np.linalg.norm(a)*np.linalg.norm(b))\n",
    "    return cs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed = []\n",
    "path = \"../texts/\" #directory in which training set is located\n",
    "files = [document for document in os.listdir(path) if document.endswith('.txt')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "numfiles = 0 #number of files in the training directory\n",
    "for file in files:\n",
    "    file.encode('utf8').strip() #encodes each of the files into utf-8\n",
    "    fh = open(os.path.join(path, file), 'r', encoding = \"utf-8\")\n",
    "    file_content = fh.read()\n",
    "    numfiles = numfiles + 1\n",
    "\n",
    "    processed.append(word_tokenize(str(normalize(file_content)))) #performing normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['inherit',\n",
       " 'basic',\n",
       " 'concept',\n",
       " 'object',\n",
       " 'orient',\n",
       " 'program',\n",
       " 'basic',\n",
       " 'idea',\n",
       " 'creat',\n",
       " 'new',\n",
       " 'class',\n",
       " 'add',\n",
       " 'extra',\n",
       " 'detail',\n",
       " 'exist',\n",
       " 'class',\n",
       " 'done',\n",
       " 'allow',\n",
       " 'new',\n",
       " 'class',\n",
       " 'reu',\n",
       " 'method',\n",
       " 'variabl',\n",
       " 'exist',\n",
       " 'class',\n",
       " 'new',\n",
       " 'method',\n",
       " 'class',\n",
       " 'ad',\n",
       " 'speciali',\n",
       " 'new',\n",
       " 'class',\n",
       " 'inherit',\n",
       " 'model',\n",
       " 'kind',\n",
       " 'relationship',\n",
       " 'entiti',\n",
       " 'object',\n",
       " 'exampl',\n",
       " 'postgradu',\n",
       " 'undergradu',\n",
       " 'kind',\n",
       " 'student',\n",
       " 'kind',\n",
       " 'relationship',\n",
       " 'visuali',\n",
       " 'tree',\n",
       " 'structur',\n",
       " 'student',\n",
       " 'would',\n",
       " 'gener',\n",
       " 'root',\n",
       " 'node',\n",
       " 'postgradu',\n",
       " 'undergradu',\n",
       " 'would',\n",
       " 'speciali',\n",
       " 'exten',\n",
       " 'student',\n",
       " 'node',\n",
       " 'child',\n",
       " 'node',\n",
       " 'relationship',\n",
       " 'student',\n",
       " 'would',\n",
       " 'known',\n",
       " 'superclass',\n",
       " 'parent',\n",
       " 'class',\n",
       " 'wherea',\n",
       " 'postgradu',\n",
       " 'would',\n",
       " 'known',\n",
       " 'subclass',\n",
       " 'child',\n",
       " 'class',\n",
       " 'postgradu',\n",
       " 'class',\n",
       " 'extend',\n",
       " 'student',\n",
       " 'class',\n",
       " 'inherit',\n",
       " 'occur',\n",
       " 'sever',\n",
       " 'layer',\n",
       " 'visuali',\n",
       " 'would',\n",
       " 'display',\n",
       " 'larger',\n",
       " 'tree',\n",
       " 'structur',\n",
       " 'exampl',\n",
       " 'could',\n",
       " 'extend',\n",
       " 'postgradu',\n",
       " 'node',\n",
       " 'ad',\n",
       " 'two',\n",
       " 'extra',\n",
       " 'extend',\n",
       " 'class',\n",
       " 'call',\n",
       " 'msc',\n",
       " 'student',\n",
       " 'phd',\n",
       " 'student',\n",
       " 'type',\n",
       " 'student',\n",
       " 'kind',\n",
       " 'postgradu',\n",
       " 'student',\n",
       " 'would',\n",
       " 'mean',\n",
       " 'msc',\n",
       " 'student',\n",
       " 'phd',\n",
       " 'student',\n",
       " 'class',\n",
       " 'would',\n",
       " 'inherit',\n",
       " 'method',\n",
       " 'variabl',\n",
       " 'postgradu',\n",
       " 'student',\n",
       " 'class']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "TFdict = [] #term frequency dictionary of the training set\n",
    "for i in range(len(processed)):\n",
    "    d = calcTFdict(processed[i])\n",
    "    TFdict.append(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'pagerank': 0.07017543859649122,\n",
       " 'link': 0.042105263157894736,\n",
       " 'analysi': 0.0035087719298245615,\n",
       " 'algorithm': 0.017543859649122806,\n",
       " 'use': 0.014035087719298246,\n",
       " 'googl': 0.042105263157894736,\n",
       " 'internet': 0.007017543859649123,\n",
       " 'search': 0.007017543859649123,\n",
       " 'engin': 0.0035087719298245615,\n",
       " 'assign': 0.014035087719298246,\n",
       " 'numer': 0.014035087719298246,\n",
       " 'weight': 0.010526315789473684,\n",
       " 'element': 0.007017543859649123,\n",
       " 'hyperlink': 0.007017543859649123,\n",
       " 'set': 0.007017543859649123,\n",
       " 'document': 0.007017543859649123,\n",
       " 'world': 0.007017543859649123,\n",
       " 'wide': 0.007017543859649123,\n",
       " 'web': 0.017543859649122806,\n",
       " 'purpo': 0.0035087719298245615,\n",
       " 'measur': 0.0035087719298245615,\n",
       " 'rel': 0.0035087719298245615,\n",
       " 'import': 0.017543859649122806,\n",
       " 'within': 0.0035087719298245615,\n",
       " 'may': 0.0035087719298245615,\n",
       " 'appli': 0.0035087719298245615,\n",
       " 'collect': 0.0035087719298245615,\n",
       " 'entiti': 0.0035087719298245615,\n",
       " 'reciproc': 0.0035087719298245615,\n",
       " 'quotat': 0.0035087719298245615,\n",
       " 'refer': 0.0035087719298245615,\n",
       " 'given': 0.0035087719298245615,\n",
       " 'also': 0.010526315789473684,\n",
       " 'call': 0.0035087719298245615,\n",
       " 'denot': 0.007017543859649123,\n",
       " 'pr': 0.0035087719298245615,\n",
       " 'name': 0.0035087719298245615,\n",
       " 'trademark': 0.0035087719298245615,\n",
       " 'process': 0.0035087719298245615,\n",
       " 'patent': 0.017543859649122806,\n",
       " '6285999': 0.0035087719298245615,\n",
       " 'howev': 0.0035087719298245615,\n",
       " 'stanford': 0.007017543859649123,\n",
       " 'univ': 0.010526315789473684,\n",
       " 'exclu': 0.0035087719298245615,\n",
       " 'licen': 0.0035087719298245615,\n",
       " 'right': 0.0035087719298245615,\n",
       " 'receiv': 0.010526315789473684,\n",
       " 'million': 0.007017543859649123,\n",
       " 'share': 0.007017543859649123,\n",
       " 'exchang': 0.0035087719298245615,\n",
       " 'sold': 0.0035087719298245615,\n",
       " '2005': 0.0035087719298245615,\n",
       " '336': 0.0035087719298245615,\n",
       " 'describ': 0.0035087719298245615,\n",
       " 'reli': 0.0035087719298245615,\n",
       " 'uniqu': 0.0035087719298245615,\n",
       " 'democrat': 0.0035087719298245615,\n",
       " 'natur': 0.0035087719298245615,\n",
       " 'vast': 0.0035087719298245615,\n",
       " 'structur': 0.0035087719298245615,\n",
       " 'indic': 0.0035087719298245615,\n",
       " 'individu': 0.0035087719298245615,\n",
       " 'page': 0.08421052631578947,\n",
       " 'valu': 0.007017543859649123,\n",
       " 'essenc': 0.0035087719298245615,\n",
       " 'interpret': 0.0035087719298245615,\n",
       " 'vote': 0.017543859649122806,\n",
       " 'look': 0.0035087719298245615,\n",
       " 'sheer': 0.0035087719298245615,\n",
       " 'volum': 0.0035087719298245615,\n",
       " 'analyz': 0.0035087719298245615,\n",
       " 'cast': 0.007017543859649123,\n",
       " 'weigh': 0.0035087719298245615,\n",
       " 'heavili': 0.0035087719298245615,\n",
       " 'help': 0.0035087719298245615,\n",
       " 'make': 0.0035087719298245615,\n",
       " 'word': 0.007017543859649123,\n",
       " 'result': 0.0035087719298245615,\n",
       " 'ballot': 0.0035087719298245615,\n",
       " 'among': 0.0035087719298245615,\n",
       " 'count': 0.0035087719298245615,\n",
       " 'support': 0.007017543859649123,\n",
       " 'defin': 0.0035087719298245615,\n",
       " 'recur': 0.0035087719298245615,\n",
       " 'depend': 0.0035087719298245615,\n",
       " 'number': 0.0035087719298245615,\n",
       " 'metric': 0.0035087719298245615,\n",
       " 'incom': 0.0035087719298245615,\n",
       " 'mani': 0.0035087719298245615,\n",
       " 'high': 0.007017543859649123,\n",
       " 'rank': 0.007017543859649123,\n",
       " '10': 0.0035087719298245615,\n",
       " 'webpag': 0.0035087719298245615,\n",
       " 'site': 0.0035087719298245615,\n",
       " 'eye': 0.0035087719298245615,\n",
       " 'deriv': 0.0035087719298245615,\n",
       " 'theoret': 0.0035087719298245615,\n",
       " 'probabl': 0.0035087719298245615,\n",
       " 'logarithm': 0.0035087719298245615,\n",
       " 'scale': 0.007017543859649123,\n",
       " 'like': 0.0035087719298245615,\n",
       " 'richter': 0.0035087719298245615,\n",
       " 'particular': 0.0035087719298245615,\n",
       " 'roughli': 0.0035087719298245615,\n",
       " 'base': 0.007017543859649123,\n",
       " 'upon': 0.0035087719298245615,\n",
       " 'quantiti': 0.0035087719298245615,\n",
       " 'inbound': 0.0035087719298245615,\n",
       " 'well': 0.0035087719298245615,\n",
       " 'provid': 0.007017543859649123,\n",
       " 'known': 0.0035087719298245615,\n",
       " 'factor': 0.007017543859649123,\n",
       " 'relev': 0.0035087719298245615,\n",
       " 'actual': 0.0035087719298245615,\n",
       " 'visit': 0.0035087719298245615,\n",
       " 'report': 0.0035087719298245615,\n",
       " 'toolbar': 0.0035087719298245615,\n",
       " 'influenc': 0.007017543859649123,\n",
       " 'order': 0.0035087719298245615,\n",
       " 'prevent': 0.0035087719298245615,\n",
       " 'manipul': 0.007017543859649123,\n",
       " 'spoof': 0.0035087719298245615,\n",
       " 'spamdex': 0.0035087719298245615,\n",
       " 'specif': 0.0035087719298245615,\n",
       " 'detail': 0.0035087719298245615,\n",
       " 'academ': 0.0035087719298245615,\n",
       " 'paper': 0.007017543859649123,\n",
       " 'concern': 0.0035087719298245615,\n",
       " 'publish': 0.0035087719298245615,\n",
       " 'sinc': 0.0035087719298245615,\n",
       " 'brin': 0.0035087719298245615,\n",
       " 'origin': 0.0035087719298245615,\n",
       " 'practic': 0.0035087719298245615,\n",
       " 'concept': 0.0035087719298245615,\n",
       " 'proven': 0.0035087719298245615,\n",
       " 'vulner': 0.0035087719298245615,\n",
       " 'exten': 0.0035087719298245615,\n",
       " 'research': 0.0035087719298245615,\n",
       " 'devot': 0.0035087719298245615,\n",
       " 'identifi': 0.0035087719298245615,\n",
       " 'fal': 0.007017543859649123,\n",
       " 'inflat': 0.007017543859649123,\n",
       " 'way': 0.0035087719298245615,\n",
       " 'ignor': 0.0035087719298245615,\n",
       " 'includ': 0.0035087719298245615,\n",
       " 'hit': 0.0035087719298245615,\n",
       " 'invent': 0.0035087719298245615,\n",
       " 'jon': 0.0035087719298245615,\n",
       " 'kleinberg': 0.0035087719298245615,\n",
       " 'teoma': 0.0035087719298245615,\n",
       " 'ask': 0.0035087719298245615,\n",
       " 'com': 0.0035087719298245615,\n",
       " 'ibm': 0.0035087719298245615,\n",
       " 'clever': 0.0035087719298245615,\n",
       " 'project': 0.0035087719298245615,\n",
       " 'trustrank': 0.0035087719298245615}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TFdict[19]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "countDict = calcCountDict() #calculating the number of documents in which each term appears"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "countDict[\"trustrank\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "IDFDict = calcIDFDict() #calculating the IDF dictionary of the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.302585092994046"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "IDFDict[\"trustrank\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "TFIDFDict = [calcTFIDFDict(doc) for doc in TFdict]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'object': 0.01174825729762619,\n",
       " 'orient': 0.0051767837958991815,\n",
       " 'program': 0.007832171531750793,\n",
       " 'inherit': 0.04307694342462937,\n",
       " 'way': 0.005931198443495354,\n",
       " 'form': 0.0051767837958991815,\n",
       " 'new': 0.013534028749453756,\n",
       " 'class': 0.03132868612700317,\n",
       " 'instanc': 0.013604212478259166,\n",
       " 'call': 0.01174825729762619,\n",
       " 'use': 0.0005795852473169545,\n",
       " 'alreadi': 0.006802106239129583,\n",
       " 'defin': 0.0051767837958991815,\n",
       " 'concept': 0.006802106239129583,\n",
       " 'invent': 0.007832171531750793,\n",
       " '1967': 0.010718192005004979,\n",
       " 'simula': 0.010718192005004979,\n",
       " 'known': 0.00902268583296917,\n",
       " 'deriv': 0.004511342916484585,\n",
       " 'take': 0.0051767837958991815,\n",
       " 'attribut': 0.004511342916484585,\n",
       " 'behavior': 0.009092869561774578,\n",
       " 'pre': 0.009092869561774578,\n",
       " 'exist': 0.00902268583296917,\n",
       " 'refer': 0.003377610173760567,\n",
       " 'base': 0.003916085765875397,\n",
       " 'ancestor': 0.01779359533048606,\n",
       " 'intend': 0.006802106239129583,\n",
       " 'help': 0.006802106239129583,\n",
       " 'reu': 0.005931198443495354,\n",
       " 'code': 0.020707135183596726,\n",
       " 'littl': 0.007832171531750793,\n",
       " 'modif': 0.006802106239129583,\n",
       " 'provid': 0.005931198443495354,\n",
       " 'support': 0.007832171531750793,\n",
       " 'repres': 0.009092869561774578,\n",
       " 'categor': 0.015664343063501587,\n",
       " 'comput': 0.005931198443495354,\n",
       " 'languag': 0.009092869561774578,\n",
       " 'power': 0.009092869561774578,\n",
       " 'mechan': 0.010718192005004979,\n",
       " 'number': 0.005931198443495354,\n",
       " 'inform': 0.018185739123549156,\n",
       " 'process': 0.0051767837958991815,\n",
       " 'crucial': 0.013008955327649977,\n",
       " 'human': 0.007832171531750793,\n",
       " 'learn': 0.009092869561774578,\n",
       " 'mean': 0.0051767837958991815,\n",
       " 'gener': 0.01174825729762619,\n",
       " 'specif': 0.018185739123549156,\n",
       " 'entiti': 0.010353567591798363,\n",
       " 'appli': 0.005931198443495354,\n",
       " 'wider': 0.010718192005004979,\n",
       " 'group': 0.009092869561774578,\n",
       " 'given': 0.005931198443495354,\n",
       " 'belong': 0.009092869561774578,\n",
       " 'relat': 0.009092869561774578,\n",
       " 'establish': 0.010718192005004979,\n",
       " 'cognit': 0.009092869561774578,\n",
       " 'economi': 0.010718192005004979,\n",
       " 'le': 0.007832171531750793,\n",
       " 'need': 0.010718192005004979,\n",
       " 'store': 0.010718192005004979,\n",
       " 'particular': 0.006802106239129583,\n",
       " 'also': 0.003916085765875397,\n",
       " 'sometim': 0.009092869561774578,\n",
       " 'relationship': 0.004511342916484585,\n",
       " 'repr': 0.005931198443495354,\n",
       " 'hierarchi': 0.006802106239129583,\n",
       " 'fruit': 0.04546434780887289,\n",
       " 'appl': 0.04546434780887289,\n",
       " 'orang': 0.018185739123549156,\n",
       " 'mango': 0.009092869561774578,\n",
       " 'mani': 0.007832171531750793,\n",
       " 'one': 0.006755220347521134,\n",
       " 'consid': 0.006802106239129583,\n",
       " 'abstract': 0.009092869561774578,\n",
       " 'etc': 0.009092869561774578,\n",
       " 'conver': 0.009092869561774578,\n",
       " 'sinc': 0.005931198443495354,\n",
       " 'may': 0.010353567591798363,\n",
       " 'natur': 0.006802106239129583,\n",
       " 'properti': 0.006802106239129583,\n",
       " 'common': 0.007832171531750793,\n",
       " 'fleshi': 0.009092869561774578,\n",
       " 'contain': 0.006802106239129583,\n",
       " 'seed': 0.009092869561774578,\n",
       " 'plant': 0.009092869561774578,\n",
       " 'advantag': 0.005931198443495354,\n",
       " 'modul': 0.006802106239129583,\n",
       " 'suffici': 0.015664343063501587,\n",
       " 'similar': 0.004511342916484585,\n",
       " 'interfac': 0.006802106239129583,\n",
       " 'share': 0.007832171531750793,\n",
       " 'lot': 0.005931198443495354,\n",
       " 'reduc': 0.005931198443495354,\n",
       " 'complex': 0.011862396886990707,\n",
       " 'therefor': 0.005931198443495354,\n",
       " 'anoth': 0.006802106239129583,\n",
       " 'view': 0.007832171531750793,\n",
       " 'dual': 0.009092869561774578,\n",
       " 'polymorph': 0.006802106239129583,\n",
       " 'describ': 0.007832171531750793,\n",
       " 'piec': 0.009092869561774578,\n",
       " 'control': 0.013604212478259166,\n",
       " 'typic': 0.009092869561774578,\n",
       " 'accomplish': 0.006802106239129583,\n",
       " 'either': 0.007832171531750793,\n",
       " 'overrid': 0.005931198443495354,\n",
       " 'replac': 0.009092869561774578,\n",
       " 'method': 0.006755220347521134,\n",
       " 'expo': 0.013604212478259166,\n",
       " 'ad': 0.0051767837958991815,\n",
       " 'within': 0.006802106239129583,\n",
       " 'design': 0.010718192005004979,\n",
       " 'matur': 0.016925041093525373,\n",
       " 'lead': 0.016925041093525373,\n",
       " 'yo': 0.033850082187050747,\n",
       " 'problem': 0.013008955327649977}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TFIDFDict[18]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "termDict = sorted(countDict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf_vector = [calc_TF_IDF_Vector(doc) for doc in TFIDFDict]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.014660651709986481,\n",
       " 0.023965858188431926,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.009631782434607489,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.04793171637686385,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.005545177444479563,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.036841361487904734,\n",
       " 0.0720873067782343,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.009631782434607489,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.01517695987908705,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.012875503299472802,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.018420680743952367,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.023965858188431926,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.023965858188431926,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.007330325854993241,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.025751006598945605,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.012776123139484346,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.018420680743952367,\n",
       " 0.04553087963726115,\n",
       " 0.0,\n",
       " 0.04793171637686385,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.005545177444479563,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.018420680743952367,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.02218070977791825,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.07368272297580947,\n",
       " 0.0,\n",
       " 0.012776123139484346,\n",
       " 0.0,\n",
       " 0.023965858188431926,\n",
       " 0.023965858188431926,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.007330325854993241,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.01434808801813489,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.018420680743952367,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.04793171637686385,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.025552246278968693,\n",
       " 0.0958634327537277,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.011090354888959125,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.023965858188431926,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.007330325854993241,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.018420680743952367,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.04793171637686385,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.1677610073190235,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.005545177444479563,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.01916418470922652,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.00839857699598942,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.023965858188431926,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.01517695987908705,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.036841361487904734,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.02218070977791825,\n",
       " 0.2210481689274284,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.012875503299472802,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.012875503299472802,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.04793171637686385,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.012875503299472802,\n",
       " 0.012875503299472802,\n",
       " 0.0,\n",
       " 0.036841361487904734,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0303539197581741,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.04793171637686385,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.018420680743952367,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.10623871915360936,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_idf_vector[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity = cosine_similarity(tf_idf_vector[1], tf_idf_vector[19])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6387857815354901"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
