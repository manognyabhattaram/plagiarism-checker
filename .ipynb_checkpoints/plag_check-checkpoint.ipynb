{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: num2words in c:\\users\\nade1l\\anaconda3_2\\lib\\site-packages (0.5.10)\n",
      "Requirement already satisfied: docopt>=0.6.2 in c:\\users\\nade1l\\anaconda3_2\\lib\\site-packages (from num2words) (0.6.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install num2words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\NaDe1L\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\NaDe1L\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\NaDe1L\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "\n",
    "from os.path import isfile\n",
    "from os.path import join\n",
    "\n",
    "import os\n",
    "from num2words import num2words\n",
    "import numpy as np\n",
    "import string\n",
    "import pandas as pd\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lowercase(data):\n",
    "    #changes the case of all characters in the document to lowercase\n",
    "    return np.char.lower(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(data):\n",
    "    #removes stopwords from the document\n",
    "    stop_words = stopwords.words('english')\n",
    "    words = word_tokenize(str(data))\n",
    "    new = \"\"\n",
    "    for word in words:\n",
    "        if word not in stop_words and len(word) > 1:\n",
    "            new = new + \" \" + word\n",
    "    return new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punct(data):\n",
    "    #removes all punctuation from the document\n",
    "    punct = \"!\\\"#$%&()*+-./:;<=>?@[\\]^_`{|}~\\n\"\n",
    "    for i in range(len(punct)):\n",
    "        data = np.char.replace(data, punct[i], ' ')\n",
    "        data = np.char.replace(data, \" \", \" \")\n",
    "    data = np.char.replace(data, ',', '')\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_apostrophes(data):\n",
    "    #removing apostrophes separately\n",
    "    data = np.char.replace(data, \"'\", \"\")\n",
    "    data = np.char.replace(data, \"â\\x80\\x98\", \"\") #removing unicode apostrophes\n",
    "    data = np.char.replace(data, \"â\\x80\\x99\", \"\")\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stemming(data):\n",
    "    #performing stemming on the tokens in the document\n",
    "    stemmer = PorterStemmer()\n",
    "    tokens = word_tokenize(str(data))\n",
    "    new = \"\"\n",
    "    for word in tokens:\n",
    "        new = new + \" \" + stemmer.stem(word)\n",
    "    return new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize(data):\n",
    "    #lemmatizing the document\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = word_tokenize(str(data))\n",
    "    new = \"\"\n",
    "    for word in tokens:\n",
    "        new = new + \" \" + lemmatizer.lemmatize(word)\n",
    "    return new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def num_to_words(data):\n",
    "    #converting nunmbers to words in the document\n",
    "    tokens = word_tokenize(str(data))\n",
    "    new = \"\"\n",
    "    for word in tokens:\n",
    "        try:\n",
    "            word = num2words(int(w))\n",
    "        except:\n",
    "            a = 0\n",
    "        new = new + \" \" + word\n",
    "    new = np.char.replace(new, \"-\", \" \")\n",
    "    return new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(data):\n",
    "    #combining all the above functions in a suitable order\n",
    "    data = lowercase(data)\n",
    "    data = remove_punct(data)\n",
    "    data = remove_apostrophes(data)\n",
    "    data = remove_stopwords(data)\n",
    "    data = num_to_words(data)\n",
    "    data = lemmatize(data)\n",
    "    data = stemming(data)\n",
    "    data = remove_punct(data)\n",
    "    data = num_to_words(data)\n",
    "    data = lemmatize(data)\n",
    "    data = stemming(data)\n",
    "    data = remove_punct(data) #done again to remove hyphens produced by num2words\n",
    "    data = remove_stopwords(data) #done agan to remove stopwords produced by num2words\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#computing tf dictionary\n",
    "\n",
    "def calcTFdict(doc):\n",
    "    \"\"\"Returns a term frequency dictionary for each document, with keys that are unique tokens in the document and values are the corresponding term frequencies\"\"\"\n",
    "    \n",
    "    TFDict = {}\n",
    "    \n",
    "    #counts number of appearances of term in document\n",
    "    for term in doc:\n",
    "        if term in TFDict.keys():\n",
    "            TFDict[term] +=1\n",
    "        else:\n",
    "            TFDict[term] = 1\n",
    "            \n",
    "    #Computing tf for each term\n",
    "    for key in TFDict:\n",
    "        TFDict[key] = TFDict[key]/len(doc)\n",
    "    \n",
    "    return TFDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcCountDict(TFdict):\n",
    "    \"\"\"Returns dictionary with keys as all the unique terms in corpus and values is the number of documents in which each term appears\"\"\"\n",
    "    \n",
    "    countDict = {}\n",
    "    \n",
    "    for doc in TFdict:\n",
    "        for term in doc:\n",
    "            if term in countDict:\n",
    "                countDict[term] +=1\n",
    "            else:\n",
    "                countDict[term] = 1\n",
    "                \n",
    "    return countDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#computing idf dictionary\n",
    "\n",
    "def calcIDFDict(countDict, numfiles):\n",
    "    \"\"\"Returns dictionary whose keys are all unique words in dataset and values are corresponding Inverted Document Frequencies\"\"\"\n",
    "    \n",
    "    IDFDict = {}\n",
    "    for term in countDict:\n",
    "        IDFDict[term] = math.log(numfiles / countDict[term])\n",
    "    \n",
    "    return IDFDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculating TF-IDF dictionary\n",
    "def calcTFIDFDict(TFDict, IDFDict):\n",
    "    \"\"\"Returns dictionary whose keys are all unique terms in the document and values are corresponding TF-IDF value\"\"\"\n",
    "    \n",
    "    TFIDFDict = {}\n",
    "    \n",
    "    #for each term in the document, multiply the tf and idf values\n",
    "    \n",
    "    for term in TFDict:\n",
    "        TFIDFDict[term] = TFDict[term] * IDFDict[term]\n",
    "\n",
    "    return TFIDFDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating TF-IDF vector (for calculating cosine similarity)\n",
    "\n",
    "def calc_TF_IDF_Vector(doc, termDict):\n",
    "    TFIDFVec = [0.0] * len(termDict)\n",
    "    \n",
    "    #for each unique term, if it is in the document, store the TF-IDF value\n",
    "    for i, term in enumerate(termDict):\n",
    "        if term in doc:\n",
    "            TFIDFVec[i] = doc[term]\n",
    "        \n",
    "    return TFIDFVec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(a, b):\n",
    "    cs = np.dot(a,b)/(np.linalg.norm(a)*np.linalg.norm(b))\n",
    "    return cs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_trg = []\n",
    "path_trg = \"../texts/\" #directory in which training set is located\n",
    "trg_files = [document for document in os.listdir(path_trg) if document.endswith('.txt')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "numfiles_trg = 0 #number of files in the training directory\n",
    "for file in trg_files:\n",
    "    file.encode('utf8').strip() #encodes each of the files into utf-8\n",
    "    fh = open(os.path.join(path_trg, file), 'r', encoding = \"utf-8\")\n",
    "    file_content = fh.read()\n",
    "    numfiles_trg = numfiles_trg + 1\n",
    "\n",
    "    normalized_trg.append(word_tokenize(str(normalize(file_content)))) #performing normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "TFdict_trg = [] #term frequency dictionary of the training set\n",
    "for i in range(len(normalized_trg)):\n",
    "    d = calcTFdict(normalized_trg[i])\n",
    "    TFdict_trg.append(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "countDict_trg = calcCountDict(TFdict_trg) #calculating the number of documents in which each term appears"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "IDFDict_trg = calcIDFDict(countDict_trg, numfiles_trg) #calculating the IDF dictionary of the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "TFIDFDict_trg = [calcTFIDFDict(doc, IDFDict_trg) for doc in TFdict_trg] #calculating the TF-IDF dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "termDict_trg = sorted(countDict_trg.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf_vector_trg = [calc_TF_IDF_Vector(doc, termDict_trg) for doc in TFIDFDict_trg]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOW CHECKING SIMILARITY AGAINST TEST SET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter file name: g4pC_taska.txt\n"
     ]
    }
   ],
   "source": [
    "test_file_name = input(\"Enter file name: \") #g4pC_taska.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_test = \"../test\"\n",
    "test_file.encode('utf8').strip() #encodes test file into UTF-8\n",
    "test_file_handle = open(os.path.join(path_test, test_file_name), 'r', encoding = \"utf-8\")\n",
    "test_file_content = test_file_handle.read()\n",
    "\n",
    "normalized_test = [(word_tokenize(str(normalize(test_file_content))))] #performing normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['object',\n",
       "  'orient',\n",
       "  'program',\n",
       "  'inherit',\n",
       "  'way',\n",
       "  'form',\n",
       "  'new',\n",
       "  'class',\n",
       "  'instanc',\n",
       "  'call',\n",
       "  'object',\n",
       "  'use',\n",
       "  'class',\n",
       "  'alreadi',\n",
       "  'defin',\n",
       "  'inherit',\n",
       "  'concept',\n",
       "  'invent',\n",
       "  '1967',\n",
       "  'simula',\n",
       "  'inherit',\n",
       "  'provid',\n",
       "  'support',\n",
       "  'repres',\n",
       "  'categor',\n",
       "  'comput',\n",
       "  'languag',\n",
       "  'categor',\n",
       "  'power',\n",
       "  'mechan',\n",
       "  'number',\n",
       "  'inform',\n",
       "  'process',\n",
       "  'crucial',\n",
       "  'human',\n",
       "  'learn',\n",
       "  'mean',\n",
       "  'gener',\n",
       "  'cognit',\n",
       "  'economi',\n",
       "  'le',\n",
       "  'inform',\n",
       "  'need',\n",
       "  'store',\n",
       "  'specif',\n",
       "  'entiti',\n",
       "  'particular',\n",
       "  'new',\n",
       "  'class',\n",
       "  'known',\n",
       "  'deriv',\n",
       "  'class',\n",
       "  'take',\n",
       "  'inherit',\n",
       "  'attribut',\n",
       "  'behavior',\n",
       "  'pre',\n",
       "  'exist',\n",
       "  'class',\n",
       "  'refer',\n",
       "  'base',\n",
       "  'class',\n",
       "  'ancestor',\n",
       "  'class',\n",
       "  'intend',\n",
       "  'help',\n",
       "  'reu',\n",
       "  'exist',\n",
       "  'code',\n",
       "  'littl',\n",
       "  'modif',\n",
       "  'inherit',\n",
       "  'also',\n",
       "  'sometim',\n",
       "  'call',\n",
       "  'gener',\n",
       "  'relationship',\n",
       "  'repr',\n",
       "  'hierarchi',\n",
       "  'class',\n",
       "  'object',\n",
       "  'instanc',\n",
       "  'fruit',\n",
       "  'gener',\n",
       "  'appl',\n",
       "  'orang',\n",
       "  'mango',\n",
       "  'mani',\n",
       "  'one',\n",
       "  'consid',\n",
       "  'fruit',\n",
       "  'abstract',\n",
       "  'appl',\n",
       "  'orang',\n",
       "  'etc',\n",
       "  'conver',\n",
       "  'sinc',\n",
       "  'appl',\n",
       "  'fruit',\n",
       "  'appl',\n",
       "  'fruit',\n",
       "  'appl',\n",
       "  'may',\n",
       "  'natur',\n",
       "  'inherit',\n",
       "  'properti',\n",
       "  'common',\n",
       "  'fruit',\n",
       "  'fleshi',\n",
       "  'contain',\n",
       "  'seed',\n",
       "  'plant',\n",
       "  'advantag',\n",
       "  'inherit',\n",
       "  'modul',\n",
       "  'suffici',\n",
       "  'similar',\n",
       "  'interfac',\n",
       "  'share',\n",
       "  'lot',\n",
       "  'code',\n",
       "  'reduc',\n",
       "  'complex',\n",
       "  'program',\n",
       "  'inherit',\n",
       "  'therefor',\n",
       "  'anoth',\n",
       "  'view',\n",
       "  'dual',\n",
       "  'call',\n",
       "  'polymorph',\n",
       "  'describ',\n",
       "  'mani',\n",
       "  'piec',\n",
       "  'code',\n",
       "  'control',\n",
       "  'share',\n",
       "  'control',\n",
       "  'code',\n",
       "  'inherit',\n",
       "  'typic',\n",
       "  'accomplish',\n",
       "  'either',\n",
       "  'overrid',\n",
       "  'replac',\n",
       "  'one',\n",
       "  'method',\n",
       "  'expo',\n",
       "  'ancestor',\n",
       "  'ad',\n",
       "  'new',\n",
       "  'method',\n",
       "  'expo',\n",
       "  'ancestor',\n",
       "  'complex',\n",
       "  'inherit',\n",
       "  'inherit',\n",
       "  'use',\n",
       "  'within',\n",
       "  'design',\n",
       "  'suffici',\n",
       "  'matur',\n",
       "  'may',\n",
       "  'lead',\n",
       "  'yo',\n",
       "  'yo',\n",
       "  'problem']]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalized_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "TFDict_test = calcTFdict(normalized_test[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "countDict_test = calcCountDict(TFDict_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "IDFDict_test = calcIDFDict(countDict_test, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'object'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-33-2910862d40f8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mTFIDFDict_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcalcTFIDFDict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mTFDict_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mIDFDict_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-14-759771aa828d>\u001b[0m in \u001b[0;36mcalcTFIDFDict\u001b[1;34m(TFDict, IDFDict)\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mterm\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mTFDict\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m         \u001b[0mTFIDFDict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mterm\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTFDict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mterm\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mIDFDict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mterm\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mTFIDFDict\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'object'"
     ]
    }
   ],
   "source": [
    "TFIDFDict_test = calcTFIDFDict(TFDict_test, IDFDict_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf_vector_test = calc_TF_IDF_Vector(TFIDFDict_test, termDict_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
