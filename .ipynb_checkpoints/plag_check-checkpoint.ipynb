{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: num2words in c:\\users\\nade1l\\anaconda3_2\\lib\\site-packages (0.5.10)\n",
      "Requirement already satisfied: docopt>=0.6.2 in c:\\users\\nade1l\\anaconda3_2\\lib\\site-packages (from num2words) (0.6.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install num2words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\NaDe1L\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\NaDe1L\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\NaDe1L\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "from os.path import isfile\n",
    "from os.path import join\n",
    "\n",
    "import os\n",
    "from num2words import num2words\n",
    "import numpy as np\n",
    "import string\n",
    "import pandas as pd\n",
    "import math\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lowercase(data):\n",
    "    #changes the case of all characters in the document to lowercase\n",
    "    return np.char.lower(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(data):\n",
    "    #removes stopwords from the document\n",
    "    stop_words = stopwords.words('english')\n",
    "    words = word_tokenize(str(data))\n",
    "    new = \"\"\n",
    "    for word in words:\n",
    "        if word not in stop_words and len(word) > 1:\n",
    "            new = new + \" \" + word\n",
    "    return new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punct(data):\n",
    "    #removes all punctuation from the document\n",
    "    punct = \"!\\\"#$%&()*+-./:;<=>?@[\\]^_`{|}~\\n\"\n",
    "    for i in range(len(punct)):\n",
    "        data = np.char.replace(data, punct[i], ' ')\n",
    "        data = np.char.replace(data, \" \", \" \")\n",
    "    data = np.char.replace(data, ',', '')\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_apostrophes(data):\n",
    "    #removing apostrophes separately\n",
    "    data = np.char.replace(data, \"'\", \"\")\n",
    "    data = np.char.replace(data, \"â\\x80\\x98\", \"\") #removing unicode apostrophes\n",
    "    data = np.char.replace(data, \"â\\x80\\x99\", \"\")\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stemming(data):\n",
    "    #performing stemming on the tokens in the document\n",
    "    stemmer = PorterStemmer()\n",
    "    tokens = word_tokenize(str(data))\n",
    "    new = \"\"\n",
    "    for word in tokens:\n",
    "        new = new + \" \" + stemmer.stem(word)\n",
    "    return new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize(data):\n",
    "    #lemmatizing the document\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = word_tokenize(str(data))\n",
    "    new = \"\"\n",
    "    for word in tokens:\n",
    "        new = new + \" \" + lemmatizer.lemmatize(word)\n",
    "    return new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def num_to_words(data):\n",
    "    #converting nunmbers to words in the document\n",
    "    tokens = word_tokenize(str(data))\n",
    "    new = \"\"\n",
    "    for word in tokens:\n",
    "        try:\n",
    "            word = num2words(int(w))\n",
    "        except:\n",
    "            a = 0\n",
    "        new = new + \" \" + word\n",
    "    new = np.char.replace(new, \"-\", \" \")\n",
    "    return new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(data):\n",
    "    #combining all the above functions in a suitable order\n",
    "    data = lowercase(data)\n",
    "    data = remove_punct(data)\n",
    "    data = remove_apostrophes(data)\n",
    "    data = remove_stopwords(data)\n",
    "    data = num_to_words(data)\n",
    "    data = lemmatize(data)\n",
    "    data = stemming(data)\n",
    "    data = remove_punct(data)\n",
    "    data = num_to_words(data)\n",
    "    data = lemmatize(data)\n",
    "    data = stemming(data)\n",
    "    data = remove_punct(data) #done again to remove hyphens produced by num2words\n",
    "    data = remove_stopwords(data) #done agan to remove stopwords produced by num2words\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#computing tf dictionary\n",
    "\n",
    "def calcTFdict(doc):\n",
    "    \"\"\"Returns a term frequency dictionary for each document, with keys that are unique tokens in the document and values are the corresponding term frequencies\"\"\"\n",
    "    \n",
    "    TFDict = {}\n",
    "    \n",
    "    #counts number of appearances of term in document\n",
    "    for term in doc:\n",
    "        if term in TFDict.keys():\n",
    "            TFDict[term] +=1\n",
    "        else:\n",
    "            TFDict[term] = 1\n",
    "            \n",
    "    #Computing tf for each term\n",
    "    for key in TFDict:\n",
    "        TFDict[key] = TFDict[key]/len(doc)\n",
    "    \n",
    "    return TFDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcCountDict(TFdict):\n",
    "    \"\"\"Returns dictionary with keys as all the unique terms in corpus and values is the number of documents in which each term appears\"\"\"\n",
    "    \n",
    "    countDict = {}\n",
    "    \n",
    "    for doc in TFdict:\n",
    "        for term in doc:\n",
    "            if term in countDict:\n",
    "                countDict[term] +=1\n",
    "            else:\n",
    "                countDict[term] = 1\n",
    "                \n",
    "    return countDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#computing idf dictionary\n",
    "\n",
    "def calcIDFDict(countDict, numfiles):\n",
    "    \"\"\"Returns dictionary whose keys are all unique words in dataset and values are corresponding Inverted Document Frequencies\"\"\"\n",
    "    \n",
    "    IDFDict = {}\n",
    "    for term in countDict:\n",
    "        IDFDict[term] = math.log(numfiles / countDict[term])\n",
    "    \n",
    "    return IDFDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculating TF-IDF dictionary\n",
    "def calcTFIDFDict(TFDict, IDFDict):\n",
    "    \"\"\"Returns dictionary whose keys are all unique terms in the document and values are corresponding TF-IDF value\"\"\"\n",
    "\n",
    "    TFIDFDict = {}\n",
    "    \n",
    "    #for each term in the document, multiply the tf and idf values\n",
    "    \n",
    "    for term in TFDict:\n",
    "        TFIDFDict[term] = TFDict[term] * IDFDict[term]\n",
    "\n",
    "    return TFIDFDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating TF-IDF vector (for calculating cosine similarity)\n",
    "\n",
    "def calc_TF_IDF_Vector(doc, termDict):\n",
    "    TFIDFVec = [0.0] * len(termDict)\n",
    "    \n",
    "    #for each unique term, if it is in the document, store the TF-IDF value\n",
    "    for i, term in enumerate(termDict):\n",
    "        if term in doc:\n",
    "            TFIDFVec[i] = doc[term]\n",
    "        \n",
    "    return TFIDFVec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dot_product(a, b):\n",
    "    #returns dot product of two vectors\n",
    "    dp = 0.0\n",
    "    for i, j in zip(a, b):\n",
    "        dp += i * j\n",
    "    return dp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def norm(vec):\n",
    "    #returns the norm or magnitude of a vector\n",
    "    n = 0.0\n",
    "    for i in vec:\n",
    "        n += math.pow(i, 2)\n",
    "    return math.sqrt(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(a, b):\n",
    "    #returns cosine similarity score of two vectors\n",
    "    cs = dot_product(a, b)/(norm(a) * norm(b))\n",
    "    return cs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ALL THE CELLS AFTER THIS POINT SHOULD BE RUN EVERY TIME THE TEST OR TRAINING SETS ARE MODIFIED. THE ABOVE CELLS NEED NOT BE RE-RUN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "normalized_trg = []\n",
    "normalized_test = []\n",
    "path_trg = \"./texts/\" #directory in which training set is located\n",
    "path_test = \"./test/\"\n",
    "#test_file = input(\"Enter file name: \") #g4pC_taska.txt\n",
    "trg_files = [document for document in os.listdir(path_trg) if document.endswith('.txt')]\n",
    "test_files = [document for document in os.listdir(path_test) if document.endswith('.txt')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "numfiles_trg = 0 #number of files in the training directory\n",
    "for file in trg_files:\n",
    "    file.encode('utf8').strip() #encodes each of the files into utf-8\n",
    "    fh = open(os.path.join(path_trg, file), 'r', encoding = \"utf-8\")\n",
    "    file_content = fh.read()\n",
    "    numfiles_trg = numfiles_trg + 1\n",
    "\n",
    "    normalized_trg.append(word_tokenize(str(normalize(file_content)))) #performing normalization on the training files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "numfiles_test = 0\n",
    "for file in test_files:\n",
    "    file.encode('utf8') #encodes each of the files into utf-8\n",
    "    fh = open(os.path.join(path_test, file), 'r', encoding = \"utf-8\")\n",
    "    file_content = fh.read()\n",
    "    numfiles_test = numfiles_test + 1\n",
    "\n",
    "    normalized_test.append(word_tokenize(str(normalize(file_content)))) #performing normalization on the test files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"adding test file to the total corpus so that we can perform TF-IDF vectorization\"\"\"\n",
    "normalized_corpus = normalized_trg + normalized_test \n",
    "test_doc_index_start = len(normalized_corpus) - numfiles_test\n",
    "numfiles = numfiles_trg + numfiles_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "TFdict = [] #term frequency dictionary of the corpus\n",
    "for i in range(len(normalized_corpus)):\n",
    "    d = calcTFdict(normalized_corpus[i])\n",
    "    TFdict.append(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "countDict = calcCountDict(TFdict)\n",
    "#calculating the number of documents in which each term appears"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "IDFDict = calcIDFDict(countDict, numfiles)\n",
    "#calculating the IDF dictionary of the corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "TFIDFDict = [calcTFIDFDict(doc, IDFDict) for doc in TFdict]\n",
    "#calculating the TF-IDF dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "termDict = sorted(countDict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf_vector = [calc_TF_IDF_Vector(doc, termDict) for doc in TFIDFDict]\n",
    "#vectorizing the TF-IDF dictionary for the corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity_scores = {new_list: [] for new_list in range(numfiles_test)}\n",
    "\"\"\"calculating the cosine similarity of each of the documents in the training set with respect to the test document\"\"\"\n",
    "for i in range(len(tf_idf_vector) - numfiles_test):\n",
    "    for j in range(numfiles_test):\n",
    "        cs = cosine_similarity(tf_idf_vector[(test_doc_index_start + j)], tf_idf_vector[i])\n",
    "        similarity_scores[j].append(cs) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "THE NEXT FEW CELLS ARE FOR PRINTING THE RANKED LIST FOR EACH OF THE TEST FILES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "ranked_dict_keys = trg_files\n",
    "ranked_dict = {new_list: [] for new_list in range(len(ranked_dict_keys))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(similarity_scores)):\n",
    "    ranked_dict_values = similarity_scores[i]\n",
    "    ranked_dict[i] = {ranked_dict_keys[i]: ranked_dict_values[i] for i in range(len(ranked_dict_keys))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " g3pC_taska.txt similarity ranking:\n",
      "\n",
      "g0pD_taska.txt : 87.02002001596964% similarity\n",
      "g0pE_taska.txt : 71.0001698480101% similarity\n",
      "orig_taska.txt : 66.7820733455498% similarity\n",
      "g2pE_taska.txt : 49.878585938496265% similarity\n",
      "g2pC_taska.txt : 27.733935056223523% similarity\n",
      "g1pA_taska.txt : 21.277374292224746% similarity\n",
      "g1pD_taska.txt : 20.29433451327853% similarity\n",
      "g0pC_taska.txt : 16.10499512909975% similarity\n",
      "g3pB_taska.txt : 9.475128528678892% similarity\n",
      "g0pA_taska.txt : 6.12357242034653% similarity\n",
      "g2pC_taskb.txt : 1.3118862117643872% similarity\n",
      "g0pC_taskb.txt : 1.0979031220187032% similarity\n",
      "orig_taskb.txt : 1.0587533549666956% similarity\n",
      "g0pA_taskb.txt : 0.9594629034675707% similarity\n",
      "g1pA_taskb.txt : 0.8729307023424736% similarity\n",
      "g1pD_taskb.txt : 0.5546804462922037% similarity\n",
      "g2pE_taskb.txt : 0.44697154425119767% similarity\n",
      "g0pE_taskb.txt : 0.3647105479820868% similarity\n",
      "g3pB_taskb.txt : 0.25404535798764316% similarity\n",
      "g0pD_taskb.txt : 0.0022721255872341574% similarity\n",
      "\n",
      " g3pC_taskb.txt similarity ranking:\n",
      "\n",
      "orig_taskb.txt : 30.633842760717055% similarity\n",
      "g1pA_taskb.txt : 26.521043789599524% similarity\n",
      "g0pA_taskb.txt : 26.251758635654877% similarity\n",
      "g2pC_taskb.txt : 22.284999969680854% similarity\n",
      "g3pB_taskb.txt : 20.39410571297528% similarity\n",
      "g0pC_taskb.txt : 18.133880592467058% similarity\n",
      "g0pE_taskb.txt : 14.760984501073635% similarity\n",
      "g1pD_taskb.txt : 11.41415803945748% similarity\n",
      "g2pE_taskb.txt : 8.728571855487449% similarity\n",
      "g0pD_taskb.txt : 8.165614420067307% similarity\n",
      "g0pA_taska.txt : 5.123236823701049% similarity\n",
      "g3pB_taska.txt : 3.440040073985055% similarity\n",
      "g1pD_taska.txt : 2.6659979647786507% similarity\n",
      "g2pE_taska.txt : 1.7404661078876091% similarity\n",
      "orig_taska.txt : 1.6323766871224072% similarity\n",
      "g0pE_taska.txt : 1.5605230130566283% similarity\n",
      "g0pD_taska.txt : 1.4238110801992934% similarity\n",
      "g1pA_taska.txt : 1.0623300253196755% similarity\n",
      "g2pC_taska.txt : 0.5717540516762628% similarity\n",
      "g0pC_taska.txt : 0.5131085193388677% similarity\n",
      "\n",
      " g4pC_taska.txt similarity ranking:\n",
      "\n",
      "orig_taska.txt : 97.37937048955523% similarity\n",
      "g0pE_taska.txt : 90.73573305792445% similarity\n",
      "g0pD_taska.txt : 78.87028907802072% similarity\n",
      "g2pE_taska.txt : 67.24485113334349% similarity\n",
      "g2pC_taska.txt : 46.301788630959614% similarity\n",
      "g1pD_taska.txt : 40.13444592269493% similarity\n",
      "g0pC_taska.txt : 30.48871189281568% similarity\n",
      "g1pA_taska.txt : 26.21056659010171% similarity\n",
      "g3pB_taska.txt : 14.172379819344535% similarity\n",
      "g0pA_taska.txt : 8.814577990113778% similarity\n",
      "orig_taskb.txt : 3.206929671695054% similarity\n",
      "g0pA_taskb.txt : 3.1991466024792294% similarity\n",
      "g0pC_taskb.txt : 3.183466897504012% similarity\n",
      "g3pB_taskb.txt : 1.9549875887739463% similarity\n",
      "g1pA_taskb.txt : 1.9536546975131055% similarity\n",
      "g2pE_taskb.txt : 1.8895236770298074% similarity\n",
      "g0pE_taskb.txt : 1.7666285750936657% similarity\n",
      "g2pC_taskb.txt : 1.7640187676009662% similarity\n",
      "g1pD_taskb.txt : 1.5158629762706413% similarity\n",
      "g0pD_taskb.txt : 0.8824443544830819% similarity\n"
     ]
    }
   ],
   "source": [
    "ranked_dict_view = {new_list: [] for new_list in range(len(similarity_scores))}\n",
    "for i in range(len(ranked_dict)):\n",
    "    if(ranked_dict[i]):\n",
    "        ranked_dict_view[i] = [ (v,k) for k,v in ranked_dict[i].items() ]\n",
    "        ranked_dict_view[i].sort(reverse = True) # natively sort tuples by first element\n",
    "        \n",
    "        print(\"\\n\", test_files[i], \"similarity ranking:\\n\")\n",
    "        for v,k in ranked_dict_view[i]:\n",
    "            print(k, \":\", str((v * 100)) + \"%\", \"similarity\")\n",
    "    else:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution time (including preprocessing) 3.685847282409668 seconds\n"
     ]
    }
   ],
   "source": [
    "total_time = time.time()-start_time\n",
    "print(\"Execution time (including preprocessing)\", total_time, \"seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
